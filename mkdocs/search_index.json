{
    "docs": [
        {
            "location": "/", 
            "text": "libStorage\n\n\nOpening up storage for all\n\n\n\n\nOverview\n\n\nlibStorage\n is an open source, platform agnostic, storage provisioning and\norchestration framework, model, and API.\n\n\nFeatures\n\n\nThe following features unique to this project make it a perfect choice for\nadding value to upstream applications by centralizing storage management:\n\n\n\n\nA standardized storage orchestration \nmodel and API\n\n\nA lightweight, reference client implementation with a minimal dependency\n  footprint\n\n\nThe ability to embed both the libStorage client and server, creating native\n  application integration opportunities\n\n\n\n\nOperations\n\n\nlibStorage\n supports the following operations:\n\n\n\n\n\n\n\n\nResource Type\n\n\nOperation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVolume\n\n\nList / Inspect\n\n\nGet detailed information about one to many volumes\n\n\n\n\n\n\n\n\nCreate / Remote\n\n\nManage the volume lifecycle\n\n\n\n\n\n\n\n\nAttach / Detach\n\n\nProvision volumes to a client\n\n\n\n\n\n\n\n\nMount / Unmount\n\n\nMake attached volumes ready-to-use, local file systems\n\n\n\n\n\n\nSnapshot\n\n\n\n\nComing soon\n\n\n\n\n\n\nStorage Pool\n\n\n\n\nComing soon\n\n\n\n\n\n\n\n\nGetting Started\n\n\nUsing libStorage can be broken down into several, distinct steps:\n\n\n\n\nConfiguring \nlibStorage\n\n\nUnderstanding the \nAPI\n\n\nIdentifying a production server and client implementation, such as\n   \nREX-Ray\n\n\n\n\nGetting Help\n\n\nTo get help with libStorage, please use \nGitHub issues\n or join the active\nconversation on the \n{code} by Dell EMC Community Slack Team\n in the #project-libStorage channel.\n\n\nThe code and documentation are released with no warranties or SLAs and are\nintended to be supported through a community driven process.", 
            "title": "Home"
        }, 
        {
            "location": "/#libstorage", 
            "text": "Opening up storage for all", 
            "title": "libStorage"
        }, 
        {
            "location": "/#overview", 
            "text": "libStorage  is an open source, platform agnostic, storage provisioning and\norchestration framework, model, and API.", 
            "title": "Overview"
        }, 
        {
            "location": "/#features", 
            "text": "The following features unique to this project make it a perfect choice for\nadding value to upstream applications by centralizing storage management:   A standardized storage orchestration  model and API  A lightweight, reference client implementation with a minimal dependency\n  footprint  The ability to embed both the libStorage client and server, creating native\n  application integration opportunities", 
            "title": "Features"
        }, 
        {
            "location": "/#operations", 
            "text": "libStorage  supports the following operations:     Resource Type  Operation  Description      Volume  List / Inspect  Get detailed information about one to many volumes     Create / Remote  Manage the volume lifecycle     Attach / Detach  Provision volumes to a client     Mount / Unmount  Make attached volumes ready-to-use, local file systems    Snapshot   Coming soon    Storage Pool   Coming soon", 
            "title": "Operations"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Using libStorage can be broken down into several, distinct steps:   Configuring  libStorage  Understanding the  API  Identifying a production server and client implementation, such as\n    REX-Ray", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-help", 
            "text": "To get help with libStorage, please use  GitHub issues  or join the active\nconversation on the  {code} by Dell EMC Community Slack Team  in the #project-libStorage channel.  The code and documentation are released with no warranties or SLAs and are\nintended to be supported through a community driven process.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/user-guide/config/", 
            "text": "Configuring libStorage\n\n\nTweak this, turn that, peek behind the curtain...\n\n\n\n\nOverview\n\n\nThis page reviews how to configure \nlibStorage\n to suit any environment,\nbeginning with the the most common use cases, exploring recommended guidelines,\nand finally, delving into the details of more advanced settings.\n\n\nClient/Server Configuration\n\n\nExcept when specified otherwise, the configuration examples below assume the\n\nlibStorage\n client and server exist on the same host. However, that is not at\nall a requirement. It is fully possible, and in fact the entire purpose of\n\nlibStorage\n, that the client and server be able to function on different\nsystems. One \nlibStorage\n server should be able to support hundreds of clients.\nYet for the sake of completeness, the examples below show both configurations\nmerged.\n\n\nWhen configuring a \nlibStorage\n client and server for different systems, there\nwill be a few differences from the examples below:\n\n\n\n\n\n\nThe examples show \nlibStorage\n configured with its server component hosted\n    on a UNIX socket. This is ideal for when the client/server exist on the same\n    host as it reduces security risks. However, in most real-world scenarios\n    the client and server are \nnot\n residing on the same host, the\n    \nlibStorage\n  server should use a TCP endpoint so it can be accessed\n    remotely.\n\n\n\n\n\n\nIn a distributed configuration the actual driver configuration sections\n    need only occur on the server-side. The entire purpose of \nlibStorage\n's\n    distributed nature is to enable clients without any knowledge of how to\n    access a storage platform the ability to connect to a remote server that\n    maintains that storage platform access information.\n\n\n\n\n\n\nBasic Configuration\n\n\nThis section outlines the most common configuration scenarios encountered by\n\nlibStorage\n's users.\n\n\nSimple\n\n\nThe first example is a simple \nlibStorage\n configuration with the VirtualBox\nstorage driver. The below example omits the host property, but the configuration\nis still valid. If the \nlibstorage.host\n property is not found, the server is\nhosted via a temporary UNIX socket file in \n/var/run/libstorage\n.\n\n\n\n\nnote\n\n\nPlease remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.\n\n\nThe example below specifies the \nvolumePath\n property as\n\n$HOME/VirtualBox/Volumes\n. While the text \n$HOME\n will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The \nvolumePath\n property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the \nlibStorage\n server is running.\n\n\nSo please, make sure to update the \nvolumePath\n property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.\n\n\nThe same goes for VirtualBox property \nendpoint\n as the VirtualBox\nweb service is not always available at \n10.0.2.2:18083\n.\n\n\n\n\nlibstorage:\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nTCP\n\n\nThe following example illustrates how to configure a \nlibStorage\n client and\nserver running on the same host. The server has one endpoint on which it is\naccessible - a single TCP port, 7979, bound to the localhost network interface.\n\n\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nTCP+TLS\n\n\nThe following example illustrates how to configure a \nlibStorage\n client and\nserver running on the same host. The server has one endpoint on which it is\naccessible - a single TCP port, 7979, bound to all of the host's network\ninterfaces. This means that the server is accessible via external clients, not\njust those running on the same host.\n\n\nBecause of the public nature of this \nlibStorage\n server, it is a good idea to\nencrypt communications between client and server.\n\n\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  client:\n    tls:\n      certFile: $HOME/.libstorage/libstorage-client.crt\n      keyFile: $HOME/.libstorage/libstorage-client.key\n      trustedCertsFile: $HOME/.libstorage/trusted-certs.crt\n  server:\n    tls:\n      certFile: /etc/libstorage/libstorage-server.crt\n      keyFile: /etc/libstorage/libstorage-server.key\n      trustedCertsFile: /etc/libstorage/trusted-certs.crt\n      clientCertRequired: true\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nPlease note that in the above example the property \nlibstorage.client\n has been\nintroduced. This property is always present, even if not explicitly specified.\nIt exists to override \nlibStorage\n properties for the client only, such as TLS\nsettings, logging, etc.\n\n\nUNIX Socket\n\n\nFor the security conscious, there is no safer way to run a client/server setup\non a single system than the option to use a UNIX socket. The socket offloads\nauthentication and relies on the file system file access to ensure authorized\nusers can use the \nlibStorage\n API.\n\n\nlibstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nIt is possible to apply TLS to the UNIX socket. Refer to the TCP+TLS section\nfor applying TLS to the UNIX sockets.\n\n\nMultiple Endpoints\n\n\nThere may be occasions when it is desirable to provide multiple ingress vectors\nfor the \nlibStorage\n API. In these situations, configuring multiple endpoints\nis the solution. The below example illustrates how to configure three endpoints:\n\n\n\n\n\n\n\n\nendpoint\n\n\nprotocol\n\n\naddress\n\n\ntls\n\n\nlocalhost only\n\n\n\n\n\n\n\n\n\n\nsock\n\n\nunix socket\n\n\n/var/run/libstorage/localhost.sock\n\n\nno\n\n\nyes\n\n\n\n\n\n\nprivate\n\n\ntcp\n\n\n127.0.0.1:7979\n\n\nno\n\n\nyes\n\n\n\n\n\n\npublic\n\n\ntcp\n\n\n*:7980\n\n\nyes\n\n\nno\n\n\n\n\n\n\n\n\nlibstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n    endpoints:\n      sock:\n        address: unix:///var/run/libstorage/localhost.sock\n      private:\n        address: tcp://127.0.0.1:7979\n      public:\n        address: tcp://:7980\n        tls:\n          certFile: /etc/libstorage/libstorage-server.crt\n          keyFile: /etc/libstorage/libstorage-server.key\n          trustedCertsFile: /etc/libstorage/trusted-certs.crt\n          clientCertRequired: true\n\n\n\n\nWith all three endpoints defined explicitly in the above example, why leave the\nproperty \nlibstorage.host\n in the configuration at all? When there are no\nendpoints defined, the \nlibStorage\n server will attempt to create a default\nendpoint using the value from the property \nlibstorage.host\n. However, even\nwhen there's at least one explicitly defined endpoint, the \nlibstorage.host\n\nproperty still serves a very important function -- it is the property used\nby the \nlibStorage\n client to determine which to which endpoint to connect.\n\n\nMultiple Services\n\n\nAll of the previous examples have used the VirtualBox storage driver as the\nsole measure of how to configure a \nlibStorage\n service. However, it is possible\nto configure many services at the same time in order to provide access to\nmultiple storage drivers of different types, or even different configurations\nof the same driver.\n\n\nThe following example demonstrates how to configure three \nlibStorage\n services:\n\n\n\n\n\n\n\n\nservice\n\n\ndriver\n\n\n\n\n\n\n\n\n\n\nvirtualbox-00\n\n\nvirtualbox\n\n\n\n\n\n\nvirtualbox-01\n\n\nvirtualbox\n\n\n\n\n\n\nscaleio\n\n\nscaleio\n\n\n\n\n\n\n\n\nNotice how the \nvirtualbox-01\n service includes an added \nintegration\n section.\nThe integration definition refers to the integration interface and parameters\nspecific to incoming requests through this layer. In this case we defined\n\nlibstorage.server.services.virtualbox-01\n with the\n\nintegration.volume.operations.create.default.size\n parameter set. This enables all\ncreate requests that come in through \nvirtualbox-01\n to have a default size of\n1GB. So although it is technically the same platform below the covers,\n\nvirtualbox-00\n requests may have different default values than those defined\nin \nvirtualbox-01\n.\n\n\nlibstorage:\n  server:\n    services:\n      virtualbox-00:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes-00\n          controllerName: SATA\n      virtualbox-01:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes-01\n          controllerName: SATA\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1 # GB\n      scaleio:\n        driver: scaleio\n        scaleio:\n          endpoint: https://gateway_ip/api\n          insecure: true\n          userName: username\n          password: password\n          systemName: tenantName\n          protectionDomainName: protectionDomainName\n          storagePoolName: storagePoolName\n\n\n\n\nA very important point to make about the relationship between services and\nendpoints is that all configured services are available on all endpoints. In\nthe future this may change, and \nlibStorage\n may support endpoint-specific\nservice definitions, but for now if a service is configured, it is accessible\nvia any of the available endpoint addresses.\n\n\nBetween the three services above, clearly one major difference is that two\nservices host one driver, VirtualBox, and the third service hosts ScaleIO.\nHowever, why two services for one driver, in this case, VirtualBox? Because,\nin addition to services being configured to host different types of drivers,\nservices can also host different driver configurations. In service\n\nvirtualbox-00\n, the volume path is \n$HOME/VirtualBox/Volumes-00\n,\nwhereas for service \nvirtualbox-01\n, the volume path is\n\n$HOME/VirtualBox/Volumes-01\n.\n\n\nLogging\n\n\nSometimes it helps to see a little more, or maybe even a little less,\ninformation in the logs. Configuring logging is quite straight-forward:\n\n\nlibstorage:\n  logging:\n    level: warn\n  server:\n    logging:\n      level: info\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nThe \nlibStorage\n configuration shown above uses a global log level of \nwarn\n,\nand a more verbose, \ninfo\n log level for just the server.\n\n\nAdvanced Configuration\n\n\nThe following sections detail every last aspect of how \nlibStorage\n works and can\nbe configured.\n\n\nEmbedded Configuration\n\n\nIf \nlibStorage\n is embedded into another application, such as\n\nREX-Ray\n, then that application may\nmanage its own configuration and supply the embedded \nlibStorage\n instance\ndirectly with a configuration object. In this scenario, the \nlibStorage\n\nconfiguration files are ignored in deference to the embedding application.\n\n\nData Directories\n\n\nThe first time \nlibStorage\n is executed it will create several directories if\nthey do not already exist:\n\n\n\n\n/etc/libstorage\n\n\n/var/log/libstorage\n\n\n/var/run/libstorage\n\n\n/var/lib/libstorage\n\n\n\n\nThe above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable \nLIBSTORAGE_HOME\n. All of the above\ndata directories will be placed in their same paths, but prefixed by the path\nspecified via \nLIBSTORAGE_HOME\n, if \nLIBSTORAGE_HOME\n is in fact specified.\n\n\nConfiguration Methods\n\n\nThere are three ways to configure \nlibStorage\n:\n\n\n\n\nCommand line options\n\n\nEnvironment variables\n\n\nConfiguration files\n\n\n\n\nThe order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.\n\n\nConfiguration Files\n\n\nThere are two \nlibStorage\n configuration files - global and user:\n\n\n\n\n/etc/libstorage/config.yml\n\n\n$HOME/.libstorage/config.yml\n\n\n\n\nPlease note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts \nlibStorage\n. And\nif \nlibStorage\n is being started as a service, then \nsudo\n is likely being used,\nwhich means that \n$HOME/.libstorage/config.yml\n won't point to \nyour\n home\ndirectory, but rather \n/root/.libstorage/config.yml\n.\n\n\nConfiguration Properties\n\n\nThe section \nConfiguration Methods\n mentions there are\nthree ways to configure libStorage: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.\n\n\nBelow is a simple configuration file that tells the \nlibStorage\n client where\nthe \nlibStorage\n server is hosted:\n\n\nlibstorage:\n  host: tcp://192.168.0.20:7979\n\n\n\n\nThe property \nlibstorage.host\n is a string. This value can also be set via\nenvironment variables or the command line, but to do so requires knowing the\nnames of the environment variables or CLI flags to use. Luckily those are very\neasy to figure out just by knowing the property names.\n\n\nAll properties that might appear in the \nlibStorage\n configuration file\nfall under some type of heading. For example, take the default configuration\nabove.\n\n\nThe rule for environment variables is as follows:\n\n\n\n\nEach nested level becomes a part of the environment variable name followed\n    by an underscore \n_\n except for the terminating part.\n\n\nThe entire environment variable name is uppercase.\n\n\n\n\nNested properties follow these rules for CLI flags:\n\n\n\n\nThe root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.\n\n\nThe remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.\n\n\nAll levels are then concatenated together.\n\n\n\n\nThe following example builds on the previous. In this case we have added logging\ndirectives to the client instance and reference how their transformation in\nthe table below the example.\n\n\n  libstorage:\n    host: tcp://192.168.0.20:7979\n    logging:\n      level: warn\n      stdout:\n      stderr:\n      httpRequests: false\n      httpResponses: false\n\n\n\n\nThe following table illustrates the transformations:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nEnvironment Variable\n\n\nCLI Flag\n\n\n\n\n\n\n\n\n\n\nlibstorage.host\n\n\nLIBSTORAGE_HOST\n\n\n--libstorageHost\n\n\n\n\n\n\nlibstorage.logging.level\n\n\nLIBSTORAGE_LOGGING_LEVEL\n\n\n--libstorageLoggingLevel\n\n\n\n\n\n\nlibstorage.logging.stdout\n\n\nLIBSTORAGE_LOGGING_STDOUT\n\n\n--libstorageLoggingStdout\n\n\n\n\n\n\nlibstorage.logging.stderr\n\n\nLIBSTORAGE_LOGGING_STDERR\n\n\n--libstorageLoggingStderr\n\n\n\n\n\n\nlibstorage.logging.httpRequests\n\n\nLIBSTORAGE_LOGGING_HTTPREQUESTS\n\n\n--libstorageLoggingHttpRequests\n\n\n\n\n\n\nlibstorage.logging.httpResponses\n\n\nLIBSTORAGE_LOGGING_HTTPRESPONSES\n\n\n--libstorageLoggingHttpResponses\n\n\n\n\n\n\n\n\nInherited Properties\n\n\nReferring to the section on defining\n\nMultiple Services\n, there is also another way\nto define the TLS settings for the external TCP endpoint. The same configuration\ncan be rewritten and simplified in the process:\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1 # GB\n  server:\n    virtualbox:\n      endpoint:       http://10.0.2.2:18083\n      tls:            false\n      controllerName: SATA\n    services:\n      virtualbox-00:\n        driver: virtualbox\n        virtualbox:\n          volumePath: $HOME/VirtualBox/Volumes-00\n      virtualbox-01:\n        driver: virtualbox\n        virtualbox:\n          volumePath: $HOME/VirtualBox/Volumes-01\n\n\n\n\nThe above example may look different than the previous one, but it's actually\nthe same with a minor tweak in order to simplify configuration.\n\n\nWhile there are still two VirtualBox services defined, \nvirtualbox-00\n and\n\nvirtualbox-01\n, neither service contains configuration information about the\nVirtualBox driver other than the \nvolumePath\n property. This is because the\nchange affected above is to take advantage of inherited properties.\n\n\nWhen a property is omitted, \nlibStorage\n traverses the configuration instance\nupwards, checking certain, predefined levels known as \"scopes\" to see if the\nproperty value exists there. All configured services represent a valid\nconfiguration scope as does \nlibstorage.server\n.\n\n\nThus when the VirtualBox driver is initialized and it checks for its properties,\nwhile the driver may only find the \nvolumePath\n property defined under the\nconfigured service scope, the property access attempt travels up the\nconfiguration stack until it hits the \nlibstorage.server\n scope where the\nremainder of the VirtualBox driver's properties \nare\n defined.\n\n\nOverriding Inherited Properties\n\n\nIt's also possible to override inherited properties as is demonstrated in the\n\nLogging configuration example\n above:\n\n\nlibstorage:\n  logging:\n    level: warn\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1 # GB\n  server:\n    logging:\n      level: info\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nNote that while the log level is defined at the root of the config, it's also\ndefined at \nlibstorage.server.logging.level\n. The latter value of \ninfo\n\noverrides the former value of \nwarn\n. Also please remember that even had the\nlatter, server-specific value of \ninfo\n not been defined, an attempt by to\naccess the log level by the server would be perfectly valid since the attempt\nwould traverse up the configuration data until it found the log level defined\nat the root of the configuration.\n\n\nLogging Configuration\n\n\nThe \nlibStorage\n log level determines the level of verbosity emitted by the\ninternal logger. The default level is \nwarn\n, but there are three other levels\nas well:\n\n\n\n\n\n\n\n\nLog Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror\n\n\nLog only errors\n\n\n\n\n\n\nwarn\n\n\nLog errors and anything out of place\n\n\n\n\n\n\ninfo\n\n\nLog errors, warnings, and workflow messages\n\n\n\n\n\n\ndebug\n\n\nLog everything\n\n\n\n\n\n\n\n\nTasks Configuration\n\n\nAll operations received by the libStorage API are immediately enqueued into a\nTask Service in order to divorce the business objective from the scope of the\nHTTP request that delivered it. If a task completes before the HTTP request\ntimes out, the result of the task is written to the HTTP response and sent to\nthe client. However, if the operation is long-lived and continues to execute\nafter the original HTTP request has timed out, the goroutine running the\noperation will finish regardless.\n\n\nIn the case of such a timeout event, the client receives an HTTP status 408 -\nRequest Timeout. The HTTP response body also includes the task ID which can\nbe used to monitor the state of the remote call. The following resource URI can\nbe used to retrieve information about a task:\n\n\nGET /tasks/${taskID}\n\n\n\n\nFor systems that experience heavy loads the task system can also be a source of\npotential resource issues. Because tasks are kept indefinitely at this point in\ntime, too many tasks over a long period of time can result in a massive memory\nconsumption, with reports of up to 50GB and more.\n\n\nThat's why the configuration property \nlibstorage.server.tasks.logTimeout\n is\navailable to adjust how long a task is logged before it is removed from memory.\nThe default value is \n0\n -- that is, do not log the task in memory at all.\n\n\nWhile this is in contradiction to the task retrieval example above --\nobviously a task cannot be retrieved if it is not retained -- testing and\nbenchmarks have shown it is too dangerous to enable task retention by default.\nInstead tasks are removed immediately upon completion.\n\n\nThe follow configuration example illustrates a libStorage server that keeps\ntasks logged for 10 minutes before purging them from memory:\n\n\nlibstorage:\n  server:\n    tasks:\n      logTimeout: 10m\n\n\n\n\nThe \nlibstorage.server.tasks.logTimeout\n property can be set to any value that\nis parseable by the Golang\n\ntime.ParseDuration\n function. For\nexample, \n1000ms\n, \n10s\n, \n5m\n, and \n1h\n are all valid values.\n\n\nDriver Configuration\n\n\nThere are three types of drivers:\n\n\n\n\nOS Drivers\n\n\nStorage Drivers\n\n\nIntegration Drivers\n\n\n\n\nOS Drivers\n\n\nOperating system (OS) drivers enable \nlibStorage\n to manage storage on\nthe underlying OS. Currently the following OS drivers are supported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\nlinux\n\n\n\n\n\n\n\n\nThe OS driver \nlinux\n is automatically activated when \nlibStorage\n is running on\nthe Linux OS.\n\n\nStorage Drivers\n\n\nStorage drivers enable \nlibStorage\n to communicate with direct-attached or\nremote storage systems. Currently the following storage drivers are supported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nDell EMC Isilon\n\n\nisilon\n\n\n\n\n\n\nDell EMC ScaleIO\n\n\nscaleio\n\n\n\n\n\n\nVirtualBox\n\n\nvirtualbox\n\n\n\n\n\n\nAWS EBS\n\n\nebs, ec2\n\n\n\n\n\n\nAWS EFS\n\n\nefs\n\n\n\n\n\n\nAWS S3FS\n\n\ns3fs\n\n\n\n\n\n\nCeph RBD\n\n\nrbd\n\n\n\n\n\n\nGCE PD\n\n\ngcepd\n\n\n\n\n\n\nAzure UD\n\n\nazureud\n\n\n\n\n\n\n\n\nThe \nlibstorage.server.libstorage.storage.driver\n property can be used to\nactivate a storage drivers. That is not a typo; the \nlibstorage\n key is repeated\nbeneath \nlibstorage.server\n. This is because configuration property paths are\nabsolute, and when nested under an architectural component, such as\n\nlibstorage.server\n, the entire key path must be replicated.\n\n\nThat said, and this may seem to contradict the last point, the storage driver\nproperty is valid \nonly\n on the server. Well, not really. Internally the\n\nlibStorage\n client uses the same configuration property to denote its own\nstorage driver. This internal storage driver is actually how the \nlibStorage\n\nclient communicates with the \nlibStorage\n server.\n\n\nIntegration Drivers\n\n\nIntegration drivers enable \nlibStorage\n to integrate with schedulers and other\nstorage consumers, such as \nDocker\n or \nMesos\n. Currently the following\nintegration drivers are supported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\nlinux\n\n\n\n\n\n\n\n\nThe integration driver \nlinux\n provides necessary functionality to enable\nmost consuming platforms to work with storage volumes.\n\n\nVolume Configuration\n\n\nThis section describes various global configuration options related to an\nintegration driver's volume operations, such as mounting and unmounting volumes.\n\n\nVolume Properties\n\n\nThe properties listed below are the global properties valid for an integration\ndriver's volume-related properties.\n\n\n\n\n\n\n\n\nparameter\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.integration.volume.operations.mount.preempt\n\n\nForcefully take control of volumes when requested\n\n\n\n\n\n\nlibstorage.integration.volume.operations.mount.path\n\n\nThe default host path for mounting volumes\n\n\n\n\n\n\nlibstorage.integration.volume.operations.mount.rootPath\n\n\nThe path within the volume to return to the integrator (ex. \n/data\n)\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.disable\n\n\nDisable the ability for a volume to be created\n\n\n\n\n\n\nlibstorage.integration.volume.operations.remove.disable\n\n\nDisable the ability for a volume to be removed\n\n\n\n\n\n\n\n\nThe properties in the next table are the configurable parameters that affect\nthe default values for volume creation requests.\n\n\n\n\n\n\n\n\nparameter\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.default.size\n\n\nSize in GB\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.default.iops\n\n\nIOPS\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.default.type\n\n\nType of Volume or Storage Pool\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.default.fsType\n\n\nType of filesystem for new volumes (ext4/xfs)\n\n\n\n\n\n\nlibstorage.integration.volume.operations.create.default.availabilityZone\n\n\nExtensible parameter per storage driver\n\n\n\n\n\n\n\n\nDisable Create\n\n\nThe disable create feature enables you to disallow any volume creation activity.\nAny requests will be returned in a successful manner, but the create will not\nget passed to the backend storage platform.\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        create:\n          disable: true\n\n\n\n\nDisable Remove\n\n\nThe disable remove feature enables you to disallow any volume removal activity.\nAny requests will be returned in a successful manner, but the remove will not\nget passed to the backend storage platform.\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        remove:\n          disable: true\n\n\n\n\nPreemption\n\n\nThere is a capability to preemptively detach any existing attachments to other\ninstances before attempting a mount.  This will enable use cases for\navailability where another instance must be able to take control of a volume\nwithout the current owner instance being involved.  The operation is considered\nequivalent to a power off of the existing instance for the device.\n\n\nExample configuration file follows:\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true\n\n\n\n\n\n\n\n\n\n\nDriver\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nDell EMC Isilon\n\n\nNot yet\n\n\n\n\n\n\nDell EMC ScaleIO\n\n\nYes\n\n\n\n\n\n\nVirtualBox\n\n\nYes\n\n\n\n\n\n\nAWS EBS\n\n\nYes\n\n\n\n\n\n\nAWS EFS\n\n\nNo\n\n\n\n\n\n\nAWS S3FS\n\n\nNo\n\n\n\n\n\n\nCeph RBD\n\n\nNo\n\n\n\n\n\n\nGCE PD\n\n\nYes\n\n\n\n\n\n\nAzure UD\n\n\nYes\n\n\n\n\n\n\n\n\nIgnore Used Count\n\n\nBy default accounting takes place during operations that are performed\non \nMount\n, \nUnmount\n, and other operations.  This only has impact when running\nas a service through the HTTP/JSON interface since the counts are persisted\nin memory.  The purpose of respecting the \nUsed Count\n is to ensure that a\nvolume is not unmounted until the unmount requests have equaled the mount\nrequests.  \n\n\nIn the \nDocker\n use case if there are multiple containers sharing a volume\non the same host, the the volume will not be unmounted until the last container\nis stopped.  \n\n\nThe following setting should only be used if you wish to \ndisable\n this\nfunctionality.  This would make sense if the accounting is being done from\nhigher layers and all unmount operations should proceed without control.\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        unmount:\n          ignoreUsedCount: true\n\n\n\n\nCurrently a reset of the service will cause the counts to be reset.  This\nwill cause issues if \nmultiple containers\n are sharing a volume.  If you are\nsharing volumes, it is recommended that you reset the service along with the\naccompanying container runtime (if this setting is false) to ensure they are\nsynchronized.  \n\n\nVolume Path Cache\n\n\nIn order to optimize \nPath\n requests, the paths of actively mounted volumes\nreturned as the result of a \nList\n request are cached. Subsequent \nPath\n\nrequests for unmounted volumes will not dirty the cache. Only once a volume\nhas been mounted will the cache be marked dirty and the volume's path retrieved\nand cached once more.\n\n\nThe following configuration example illustrates the two path cache properties:\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        path:\n          cache:\n            enabled: true\n            async:   true\n\n\n\n\nVolume path caching is enabled and asynchronous by default, so it's possible to\nentirely omit the above configuration excerpt from a production deployment, and\nthe system will still use asynchronous caching. Setting the \nasync\n property to\n\nfalse\n simply means that the initial population of the cache will be handled\nsynchronously, slowing down the program's startup time.\n\n\nVolume Root Path\n\n\nWhen volumes are mounted there can be an additional path that is specified to\nbe created and passed as the valid mount point.  This is required for certain\napplications that do not want to place data from the root of a mount point.\n\n\nThe default is the \n/data\n path.  If a value is set by\n\nlinux.integration.volume.operations.mount.rootPath\n, then the default will be\noverwritten.\n\n\nlibstorage:\n  integration:\n    volume:\n      operations:\n        mount:\n          rootPath: /data", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/config/#configuring-libstorage", 
            "text": "Tweak this, turn that, peek behind the curtain...", 
            "title": "Configuring libStorage"
        }, 
        {
            "location": "/user-guide/config/#overview", 
            "text": "This page reviews how to configure  libStorage  to suit any environment,\nbeginning with the the most common use cases, exploring recommended guidelines,\nand finally, delving into the details of more advanced settings.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/config/#clientserver-configuration", 
            "text": "Except when specified otherwise, the configuration examples below assume the libStorage  client and server exist on the same host. However, that is not at\nall a requirement. It is fully possible, and in fact the entire purpose of libStorage , that the client and server be able to function on different\nsystems. One  libStorage  server should be able to support hundreds of clients.\nYet for the sake of completeness, the examples below show both configurations\nmerged.  When configuring a  libStorage  client and server for different systems, there\nwill be a few differences from the examples below:    The examples show  libStorage  configured with its server component hosted\n    on a UNIX socket. This is ideal for when the client/server exist on the same\n    host as it reduces security risks. However, in most real-world scenarios\n    the client and server are  not  residing on the same host, the\n     libStorage   server should use a TCP endpoint so it can be accessed\n    remotely.    In a distributed configuration the actual driver configuration sections\n    need only occur on the server-side. The entire purpose of  libStorage 's\n    distributed nature is to enable clients without any knowledge of how to\n    access a storage platform the ability to connect to a remote server that\n    maintains that storage platform access information.", 
            "title": "Client/Server Configuration"
        }, 
        {
            "location": "/user-guide/config/#basic-configuration", 
            "text": "This section outlines the most common configuration scenarios encountered by libStorage 's users.", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/user-guide/config/#simple", 
            "text": "The first example is a simple  libStorage  configuration with the VirtualBox\nstorage driver. The below example omits the host property, but the configuration\nis still valid. If the  libstorage.host  property is not found, the server is\nhosted via a temporary UNIX socket file in  /var/run/libstorage .   note  Please remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.  The example below specifies the  volumePath  property as $HOME/VirtualBox/Volumes . While the text  $HOME  will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The  volumePath  property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the  libStorage  server is running.  So please, make sure to update the  volumePath  property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.  The same goes for VirtualBox property  endpoint  as the VirtualBox\nweb service is not always available at  10.0.2.2:18083 .   libstorage:\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA", 
            "title": "Simple"
        }, 
        {
            "location": "/user-guide/config/#tcp", 
            "text": "The following example illustrates how to configure a  libStorage  client and\nserver running on the same host. The server has one endpoint on which it is\naccessible - a single TCP port, 7979, bound to the localhost network interface.  libstorage:\n  host: tcp://127.0.0.1:7979\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA", 
            "title": "TCP"
        }, 
        {
            "location": "/user-guide/config/#tcptls", 
            "text": "The following example illustrates how to configure a  libStorage  client and\nserver running on the same host. The server has one endpoint on which it is\naccessible - a single TCP port, 7979, bound to all of the host's network\ninterfaces. This means that the server is accessible via external clients, not\njust those running on the same host.  Because of the public nature of this  libStorage  server, it is a good idea to\nencrypt communications between client and server.  libstorage:\n  host: tcp://127.0.0.1:7979\n  client:\n    tls:\n      certFile: $HOME/.libstorage/libstorage-client.crt\n      keyFile: $HOME/.libstorage/libstorage-client.key\n      trustedCertsFile: $HOME/.libstorage/trusted-certs.crt\n  server:\n    tls:\n      certFile: /etc/libstorage/libstorage-server.crt\n      keyFile: /etc/libstorage/libstorage-server.key\n      trustedCertsFile: /etc/libstorage/trusted-certs.crt\n      clientCertRequired: true\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA  Please note that in the above example the property  libstorage.client  has been\nintroduced. This property is always present, even if not explicitly specified.\nIt exists to override  libStorage  properties for the client only, such as TLS\nsettings, logging, etc.", 
            "title": "TCP+TLS"
        }, 
        {
            "location": "/user-guide/config/#unix-socket", 
            "text": "For the security conscious, there is no safer way to run a client/server setup\non a single system than the option to use a UNIX socket. The socket offloads\nauthentication and relies on the file system file access to ensure authorized\nusers can use the  libStorage  API.  libstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA  It is possible to apply TLS to the UNIX socket. Refer to the TCP+TLS section\nfor applying TLS to the UNIX sockets.", 
            "title": "UNIX Socket"
        }, 
        {
            "location": "/user-guide/config/#multiple-endpoints", 
            "text": "There may be occasions when it is desirable to provide multiple ingress vectors\nfor the  libStorage  API. In these situations, configuring multiple endpoints\nis the solution. The below example illustrates how to configure three endpoints:     endpoint  protocol  address  tls  localhost only      sock  unix socket  /var/run/libstorage/localhost.sock  no  yes    private  tcp  127.0.0.1:7979  no  yes    public  tcp  *:7980  yes  no     libstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n    endpoints:\n      sock:\n        address: unix:///var/run/libstorage/localhost.sock\n      private:\n        address: tcp://127.0.0.1:7979\n      public:\n        address: tcp://:7980\n        tls:\n          certFile: /etc/libstorage/libstorage-server.crt\n          keyFile: /etc/libstorage/libstorage-server.key\n          trustedCertsFile: /etc/libstorage/trusted-certs.crt\n          clientCertRequired: true  With all three endpoints defined explicitly in the above example, why leave the\nproperty  libstorage.host  in the configuration at all? When there are no\nendpoints defined, the  libStorage  server will attempt to create a default\nendpoint using the value from the property  libstorage.host . However, even\nwhen there's at least one explicitly defined endpoint, the  libstorage.host \nproperty still serves a very important function -- it is the property used\nby the  libStorage  client to determine which to which endpoint to connect.", 
            "title": "Multiple Endpoints"
        }, 
        {
            "location": "/user-guide/config/#multiple-services", 
            "text": "All of the previous examples have used the VirtualBox storage driver as the\nsole measure of how to configure a  libStorage  service. However, it is possible\nto configure many services at the same time in order to provide access to\nmultiple storage drivers of different types, or even different configurations\nof the same driver.  The following example demonstrates how to configure three  libStorage  services:     service  driver      virtualbox-00  virtualbox    virtualbox-01  virtualbox    scaleio  scaleio     Notice how the  virtualbox-01  service includes an added  integration  section.\nThe integration definition refers to the integration interface and parameters\nspecific to incoming requests through this layer. In this case we defined libstorage.server.services.virtualbox-01  with the integration.volume.operations.create.default.size  parameter set. This enables all\ncreate requests that come in through  virtualbox-01  to have a default size of\n1GB. So although it is technically the same platform below the covers, virtualbox-00  requests may have different default values than those defined\nin  virtualbox-01 .  libstorage:\n  server:\n    services:\n      virtualbox-00:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes-00\n          controllerName: SATA\n      virtualbox-01:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes-01\n          controllerName: SATA\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1 # GB\n      scaleio:\n        driver: scaleio\n        scaleio:\n          endpoint: https://gateway_ip/api\n          insecure: true\n          userName: username\n          password: password\n          systemName: tenantName\n          protectionDomainName: protectionDomainName\n          storagePoolName: storagePoolName  A very important point to make about the relationship between services and\nendpoints is that all configured services are available on all endpoints. In\nthe future this may change, and  libStorage  may support endpoint-specific\nservice definitions, but for now if a service is configured, it is accessible\nvia any of the available endpoint addresses.  Between the three services above, clearly one major difference is that two\nservices host one driver, VirtualBox, and the third service hosts ScaleIO.\nHowever, why two services for one driver, in this case, VirtualBox? Because,\nin addition to services being configured to host different types of drivers,\nservices can also host different driver configurations. In service virtualbox-00 , the volume path is  $HOME/VirtualBox/Volumes-00 ,\nwhereas for service  virtualbox-01 , the volume path is $HOME/VirtualBox/Volumes-01 .", 
            "title": "Multiple Services"
        }, 
        {
            "location": "/user-guide/config/#logging", 
            "text": "Sometimes it helps to see a little more, or maybe even a little less,\ninformation in the logs. Configuring logging is quite straight-forward:  libstorage:\n  logging:\n    level: warn\n  server:\n    logging:\n      level: info\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA  The  libStorage  configuration shown above uses a global log level of  warn ,\nand a more verbose,  info  log level for just the server.", 
            "title": "Logging"
        }, 
        {
            "location": "/user-guide/config/#advanced-configuration", 
            "text": "The following sections detail every last aspect of how  libStorage  works and can\nbe configured.", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/user-guide/config/#embedded-configuration", 
            "text": "If  libStorage  is embedded into another application, such as REX-Ray , then that application may\nmanage its own configuration and supply the embedded  libStorage  instance\ndirectly with a configuration object. In this scenario, the  libStorage \nconfiguration files are ignored in deference to the embedding application.", 
            "title": "Embedded Configuration"
        }, 
        {
            "location": "/user-guide/config/#data-directories", 
            "text": "The first time  libStorage  is executed it will create several directories if\nthey do not already exist:   /etc/libstorage  /var/log/libstorage  /var/run/libstorage  /var/lib/libstorage   The above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable  LIBSTORAGE_HOME . All of the above\ndata directories will be placed in their same paths, but prefixed by the path\nspecified via  LIBSTORAGE_HOME , if  LIBSTORAGE_HOME  is in fact specified.", 
            "title": "Data Directories"
        }, 
        {
            "location": "/user-guide/config/#configuration-methods", 
            "text": "There are three ways to configure  libStorage :   Command line options  Environment variables  Configuration files   The order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.", 
            "title": "Configuration Methods"
        }, 
        {
            "location": "/user-guide/config/#configuration-files", 
            "text": "There are two  libStorage  configuration files - global and user:   /etc/libstorage/config.yml  $HOME/.libstorage/config.yml   Please note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts  libStorage . And\nif  libStorage  is being started as a service, then  sudo  is likely being used,\nwhich means that  $HOME/.libstorage/config.yml  won't point to  your  home\ndirectory, but rather  /root/.libstorage/config.yml .", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/user-guide/config/#configuration-properties", 
            "text": "The section  Configuration Methods  mentions there are\nthree ways to configure libStorage: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.  Below is a simple configuration file that tells the  libStorage  client where\nthe  libStorage  server is hosted:  libstorage:\n  host: tcp://192.168.0.20:7979  The property  libstorage.host  is a string. This value can also be set via\nenvironment variables or the command line, but to do so requires knowing the\nnames of the environment variables or CLI flags to use. Luckily those are very\neasy to figure out just by knowing the property names.  All properties that might appear in the  libStorage  configuration file\nfall under some type of heading. For example, take the default configuration\nabove.  The rule for environment variables is as follows:   Each nested level becomes a part of the environment variable name followed\n    by an underscore  _  except for the terminating part.  The entire environment variable name is uppercase.   Nested properties follow these rules for CLI flags:   The root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.  The remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.  All levels are then concatenated together.   The following example builds on the previous. In this case we have added logging\ndirectives to the client instance and reference how their transformation in\nthe table below the example.    libstorage:\n    host: tcp://192.168.0.20:7979\n    logging:\n      level: warn\n      stdout:\n      stderr:\n      httpRequests: false\n      httpResponses: false  The following table illustrates the transformations:     Property Name  Environment Variable  CLI Flag      libstorage.host  LIBSTORAGE_HOST  --libstorageHost    libstorage.logging.level  LIBSTORAGE_LOGGING_LEVEL  --libstorageLoggingLevel    libstorage.logging.stdout  LIBSTORAGE_LOGGING_STDOUT  --libstorageLoggingStdout    libstorage.logging.stderr  LIBSTORAGE_LOGGING_STDERR  --libstorageLoggingStderr    libstorage.logging.httpRequests  LIBSTORAGE_LOGGING_HTTPREQUESTS  --libstorageLoggingHttpRequests    libstorage.logging.httpResponses  LIBSTORAGE_LOGGING_HTTPRESPONSES  --libstorageLoggingHttpResponses", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/config/#inherited-properties", 
            "text": "Referring to the section on defining Multiple Services , there is also another way\nto define the TLS settings for the external TCP endpoint. The same configuration\ncan be rewritten and simplified in the process:  libstorage:\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1 # GB\n  server:\n    virtualbox:\n      endpoint:       http://10.0.2.2:18083\n      tls:            false\n      controllerName: SATA\n    services:\n      virtualbox-00:\n        driver: virtualbox\n        virtualbox:\n          volumePath: $HOME/VirtualBox/Volumes-00\n      virtualbox-01:\n        driver: virtualbox\n        virtualbox:\n          volumePath: $HOME/VirtualBox/Volumes-01  The above example may look different than the previous one, but it's actually\nthe same with a minor tweak in order to simplify configuration.  While there are still two VirtualBox services defined,  virtualbox-00  and virtualbox-01 , neither service contains configuration information about the\nVirtualBox driver other than the  volumePath  property. This is because the\nchange affected above is to take advantage of inherited properties.  When a property is omitted,  libStorage  traverses the configuration instance\nupwards, checking certain, predefined levels known as \"scopes\" to see if the\nproperty value exists there. All configured services represent a valid\nconfiguration scope as does  libstorage.server .  Thus when the VirtualBox driver is initialized and it checks for its properties,\nwhile the driver may only find the  volumePath  property defined under the\nconfigured service scope, the property access attempt travels up the\nconfiguration stack until it hits the  libstorage.server  scope where the\nremainder of the VirtualBox driver's properties  are  defined.", 
            "title": "Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#overriding-inherited-properties", 
            "text": "It's also possible to override inherited properties as is demonstrated in the Logging configuration example  above:  libstorage:\n  logging:\n    level: warn\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1 # GB\n  server:\n    logging:\n      level: info\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA  Note that while the log level is defined at the root of the config, it's also\ndefined at  libstorage.server.logging.level . The latter value of  info \noverrides the former value of  warn . Also please remember that even had the\nlatter, server-specific value of  info  not been defined, an attempt by to\naccess the log level by the server would be perfectly valid since the attempt\nwould traverse up the configuration data until it found the log level defined\nat the root of the configuration.", 
            "title": "Overriding Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#logging-configuration", 
            "text": "The  libStorage  log level determines the level of verbosity emitted by the\ninternal logger. The default level is  warn , but there are three other levels\nas well:     Log Level  Description      error  Log only errors    warn  Log errors and anything out of place    info  Log errors, warnings, and workflow messages    debug  Log everything", 
            "title": "Logging Configuration"
        }, 
        {
            "location": "/user-guide/config/#tasks-configuration", 
            "text": "All operations received by the libStorage API are immediately enqueued into a\nTask Service in order to divorce the business objective from the scope of the\nHTTP request that delivered it. If a task completes before the HTTP request\ntimes out, the result of the task is written to the HTTP response and sent to\nthe client. However, if the operation is long-lived and continues to execute\nafter the original HTTP request has timed out, the goroutine running the\noperation will finish regardless.  In the case of such a timeout event, the client receives an HTTP status 408 -\nRequest Timeout. The HTTP response body also includes the task ID which can\nbe used to monitor the state of the remote call. The following resource URI can\nbe used to retrieve information about a task:  GET /tasks/${taskID}  For systems that experience heavy loads the task system can also be a source of\npotential resource issues. Because tasks are kept indefinitely at this point in\ntime, too many tasks over a long period of time can result in a massive memory\nconsumption, with reports of up to 50GB and more.  That's why the configuration property  libstorage.server.tasks.logTimeout  is\navailable to adjust how long a task is logged before it is removed from memory.\nThe default value is  0  -- that is, do not log the task in memory at all.  While this is in contradiction to the task retrieval example above --\nobviously a task cannot be retrieved if it is not retained -- testing and\nbenchmarks have shown it is too dangerous to enable task retention by default.\nInstead tasks are removed immediately upon completion.  The follow configuration example illustrates a libStorage server that keeps\ntasks logged for 10 minutes before purging them from memory:  libstorage:\n  server:\n    tasks:\n      logTimeout: 10m  The  libstorage.server.tasks.logTimeout  property can be set to any value that\nis parseable by the Golang time.ParseDuration  function. For\nexample,  1000ms ,  10s ,  5m , and  1h  are all valid values.", 
            "title": "Tasks Configuration"
        }, 
        {
            "location": "/user-guide/config/#driver-configuration", 
            "text": "There are three types of drivers:   OS Drivers  Storage Drivers  Integration Drivers", 
            "title": "Driver Configuration"
        }, 
        {
            "location": "/user-guide/config/#os-drivers", 
            "text": "Operating system (OS) drivers enable  libStorage  to manage storage on\nthe underlying OS. Currently the following OS drivers are supported:     Driver  Driver Name      Linux  linux     The OS driver  linux  is automatically activated when  libStorage  is running on\nthe Linux OS.", 
            "title": "OS Drivers"
        }, 
        {
            "location": "/user-guide/config/#storage-drivers", 
            "text": "Storage drivers enable  libStorage  to communicate with direct-attached or\nremote storage systems. Currently the following storage drivers are supported:     Driver  Driver Name      Dell EMC Isilon  isilon    Dell EMC ScaleIO  scaleio    VirtualBox  virtualbox    AWS EBS  ebs, ec2    AWS EFS  efs    AWS S3FS  s3fs    Ceph RBD  rbd    GCE PD  gcepd    Azure UD  azureud     The  libstorage.server.libstorage.storage.driver  property can be used to\nactivate a storage drivers. That is not a typo; the  libstorage  key is repeated\nbeneath  libstorage.server . This is because configuration property paths are\nabsolute, and when nested under an architectural component, such as libstorage.server , the entire key path must be replicated.  That said, and this may seem to contradict the last point, the storage driver\nproperty is valid  only  on the server. Well, not really. Internally the libStorage  client uses the same configuration property to denote its own\nstorage driver. This internal storage driver is actually how the  libStorage \nclient communicates with the  libStorage  server.", 
            "title": "Storage Drivers"
        }, 
        {
            "location": "/user-guide/config/#integration-drivers", 
            "text": "Integration drivers enable  libStorage  to integrate with schedulers and other\nstorage consumers, such as  Docker  or  Mesos . Currently the following\nintegration drivers are supported:     Driver  Driver Name      Linux  linux     The integration driver  linux  provides necessary functionality to enable\nmost consuming platforms to work with storage volumes.", 
            "title": "Integration Drivers"
        }, 
        {
            "location": "/user-guide/config/#volume-configuration", 
            "text": "This section describes various global configuration options related to an\nintegration driver's volume operations, such as mounting and unmounting volumes.", 
            "title": "Volume Configuration"
        }, 
        {
            "location": "/user-guide/config/#volume-properties", 
            "text": "The properties listed below are the global properties valid for an integration\ndriver's volume-related properties.     parameter  description      libstorage.integration.volume.operations.mount.preempt  Forcefully take control of volumes when requested    libstorage.integration.volume.operations.mount.path  The default host path for mounting volumes    libstorage.integration.volume.operations.mount.rootPath  The path within the volume to return to the integrator (ex.  /data )    libstorage.integration.volume.operations.create.disable  Disable the ability for a volume to be created    libstorage.integration.volume.operations.remove.disable  Disable the ability for a volume to be removed     The properties in the next table are the configurable parameters that affect\nthe default values for volume creation requests.     parameter  description      libstorage.integration.volume.operations.create.default.size  Size in GB    libstorage.integration.volume.operations.create.default.iops  IOPS    libstorage.integration.volume.operations.create.default.type  Type of Volume or Storage Pool    libstorage.integration.volume.operations.create.default.fsType  Type of filesystem for new volumes (ext4/xfs)    libstorage.integration.volume.operations.create.default.availabilityZone  Extensible parameter per storage driver", 
            "title": "Volume Properties"
        }, 
        {
            "location": "/user-guide/config/#disable-create", 
            "text": "The disable create feature enables you to disallow any volume creation activity.\nAny requests will be returned in a successful manner, but the create will not\nget passed to the backend storage platform.  libstorage:\n  integration:\n    volume:\n      operations:\n        create:\n          disable: true", 
            "title": "Disable Create"
        }, 
        {
            "location": "/user-guide/config/#disable-remove", 
            "text": "The disable remove feature enables you to disallow any volume removal activity.\nAny requests will be returned in a successful manner, but the remove will not\nget passed to the backend storage platform.  libstorage:\n  integration:\n    volume:\n      operations:\n        remove:\n          disable: true", 
            "title": "Disable Remove"
        }, 
        {
            "location": "/user-guide/config/#preemption", 
            "text": "There is a capability to preemptively detach any existing attachments to other\ninstances before attempting a mount.  This will enable use cases for\navailability where another instance must be able to take control of a volume\nwithout the current owner instance being involved.  The operation is considered\nequivalent to a power off of the existing instance for the device.  Example configuration file follows:  libstorage:\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true     Driver  Supported      Dell EMC Isilon  Not yet    Dell EMC ScaleIO  Yes    VirtualBox  Yes    AWS EBS  Yes    AWS EFS  No    AWS S3FS  No    Ceph RBD  No    GCE PD  Yes    Azure UD  Yes", 
            "title": "Preemption"
        }, 
        {
            "location": "/user-guide/config/#ignore-used-count", 
            "text": "By default accounting takes place during operations that are performed\non  Mount ,  Unmount , and other operations.  This only has impact when running\nas a service through the HTTP/JSON interface since the counts are persisted\nin memory.  The purpose of respecting the  Used Count  is to ensure that a\nvolume is not unmounted until the unmount requests have equaled the mount\nrequests.    In the  Docker  use case if there are multiple containers sharing a volume\non the same host, the the volume will not be unmounted until the last container\nis stopped.    The following setting should only be used if you wish to  disable  this\nfunctionality.  This would make sense if the accounting is being done from\nhigher layers and all unmount operations should proceed without control.  libstorage:\n  integration:\n    volume:\n      operations:\n        unmount:\n          ignoreUsedCount: true  Currently a reset of the service will cause the counts to be reset.  This\nwill cause issues if  multiple containers  are sharing a volume.  If you are\nsharing volumes, it is recommended that you reset the service along with the\naccompanying container runtime (if this setting is false) to ensure they are\nsynchronized.", 
            "title": "Ignore Used Count"
        }, 
        {
            "location": "/user-guide/config/#volume-path-cache", 
            "text": "In order to optimize  Path  requests, the paths of actively mounted volumes\nreturned as the result of a  List  request are cached. Subsequent  Path \nrequests for unmounted volumes will not dirty the cache. Only once a volume\nhas been mounted will the cache be marked dirty and the volume's path retrieved\nand cached once more.  The following configuration example illustrates the two path cache properties:  libstorage:\n  integration:\n    volume:\n      operations:\n        path:\n          cache:\n            enabled: true\n            async:   true  Volume path caching is enabled and asynchronous by default, so it's possible to\nentirely omit the above configuration excerpt from a production deployment, and\nthe system will still use asynchronous caching. Setting the  async  property to false  simply means that the initial population of the cache will be handled\nsynchronously, slowing down the program's startup time.", 
            "title": "Volume Path Cache"
        }, 
        {
            "location": "/user-guide/config/#volume-root-path", 
            "text": "When volumes are mounted there can be an additional path that is specified to\nbe created and passed as the valid mount point.  This is required for certain\napplications that do not want to place data from the root of a mount point.  The default is the  /data  path.  If a value is set by linux.integration.volume.operations.mount.rootPath , then the default will be\noverwritten.  libstorage:\n  integration:\n    volume:\n      operations:\n        mount:\n          rootPath: /data", 
            "title": "Volume Root Path"
        }, 
        {
            "location": "/user-guide/storage-providers/", 
            "text": "Storage Providers\n\n\nConnecting storage and platforms...\n\n\n\n\nOverview\n\n\nThis page reviews the storage providers and platforms supported by \nlibStorage\n.\n\n\nClient/Server Configuration\n\n\nRegarding the examples below, please\n\nread the provision\n about\nclient/server configurations before proceeding.\n\n\nAmazon\n\n\nlibStorage includes support for multiple Amazon Web Services (AWS) storage\nservices.\n\n\n\n\nElastic Block Storage\n\n\nThe AWS EBS driver registers a storage driver named \nebs\n with the\nlibStorage service registry and is used to connect and manage AWS Elastic Block\nStorage volumes for EC2 instances.\n\n\n\n\nNote\n\n\nFor backwards compatibility, the driver also registers a storage driver\nnamed \nec2\n. The use of \nec2\n in config files is deprecated but functional.\n\n\n\n\n\n\nNote\n\n\nThe EBS driver does not yet support snapshots or tags, as previously supported\nin Rex-Ray v0.3.3.\n\n\n\n\nThe EBS driver is made possible by the\n\nofficial Amazon Go AWS SDK\n.\n\n\nRequirements\n\n\n\n\nAWS account\n\n\nVPC - EBS can be accessed within VPC\n\n\nAWS Credentials\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nebs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  region:         us-east-1\n  kmsKeyID:       arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef\n\n\n\n\nConfiguration Notes\n\n\n\n\nThe \naccessKey\n and \nsecretKey\n configuration parameters are optional and should\nbe used when explicit AWS credentials configuration needs to be provided. EBS driver\nuses official golang AWS SDK library and supports all other ways of providing\naccess credentials, like environment variables or instance profile IAM permissions.\n\n\nregion\n represents AWS region where EBS volumes should be provisioned.\nSee official AWS documentation for list of supported regions.\n\n\n\nIf the \nkmsKeyID\n field is specified it will be used as the encryption key for\nall volumes that are created with a truthy encryption request field.\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\n\n\n\nActivating the Driver\n\n\nTo activate the AWS EBS driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nebs\n as the driver name.\n\n\nTroubleshooting\n\n\n\n\nMake sure that AWS credentials (user or role) has following AWS permissions on\n  \nlibStorage\n server instance that will be making calls to AWS API:\n\n\nec2:AttachVolume\n,\n\n\nec2:CreateVolume\n,\n\n\nec2:CreateSnapshot\n,\n\n\nec2:CreateTags\n,\n\n\nec2:DeleteVolume\n,\n\n\nec2:DeleteSnapshot\n,\n\n\nec2:DescribeAvailabilityZones\n,\n\n\nec2:DescribeInstances\n,\n\n\nec2:DescribeVolumes\n,\n\n\nec2:DescribeVolumeAttribute\n,\n\n\nec2:DescribeVolumeStatus\n,\n\n\nec2:DescribeSnapshots\n,\n\n\nec2:CopySnapshot\n,\n\n\nec2:DescribeSnapshotAttribute\n,\n\n\nec2:DetachVolume\n,\n\n\nec2:ModifySnapshotAttribute\n,\n\n\nec2:ModifyVolumeAttribute\n,\n\n\nec2:DescribeTags\n\n\n\n\n\n\n\n\nExamples\n\n\nBelow is a working \nconfig.yml\n file that works with AWS EBS.\n\n\nlibstorage:\n  server:\n    services:\n      ebs:\n        driver: ebs\n        ebs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX\n          region:         us-east-1\n\n\n\n\n\n\nElastic File System\n\n\nThe AWS EFS driver registers a storage driver named \nefs\n with the\nlibStorage service registry and is used to connect and manage AWS Elastic File\nSystems.\n\n\nRequirements\n\n\n\n\nAWS account\n\n\nVPC - EFS can be accessed within VPC\n\n\nAWS Credentials\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nefs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  securityGroups:\n  - sg-XXXXXXX\n  - sg-XXXXXX0\n  - sg-XXXXXX1\n  region:              us-east-1\n  tag:                 test\n  disableSessionCache: false\n\n\n\n\nConfiguration Notes\n\n\n\n\nThe \naccessKey\n and \nsecretKey\n configuration parameters are optional and should\nbe used when explicit AWS credentials configuration needs to be provided. EFS driver\nuses official golang AWS SDK library and supports all other ways of providing\naccess credentials, like environment variables or instance profile IAM permissions.\n\n\nregion\n represents AWS region where EFS should be provisioned. See official AWS\ndocumentation for list of supported regions.\n\n\nsecurityGroups\n list of security groups attached to \nMountPoint\n instances.\nIf no security groups are provided the default VPC security group is used.\n\n\ntag\n is used to partition multiple services within single AWS account and is\nused as prefix for EFS names in format \n[tagprefix]/volumeName\n.\n\n\ndisableSessionCache\n is a flag that can be used to disable the session cache.\nIf the session cache is disabled then a new AWS connection is established with\nevery API call.\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nRuntime Behavior\n\n\nAWS EFS storage driver creates one EFS FileSystem per volume and provides root\nof the filesystem as NFS mount point. Volumes aren't attached to instances\ndirectly but rather exposed to each subnet by creating \nMountPoint\n in each VPC\nsubnet. When detaching volume from instance no action is taken as there isn't\ngood way to figure out if there are other instances in same subnet using\n\nMountPoint\n that is being detached. There is no charge for \nMountPoint\n\nso they are removed only once whole volume is deleted.\n\n\nBy default all EFS instances are provisioned as \ngeneralPurpose\n performance mode.\n\nmaxIO\n EFS type can be provisioned by providing \nmaxIO\n flag as \nvolumetype\n.\n\n\nIts possible to mount same volume to multiple container on a single EC2 instance\nas well as use single volume across multiple EC2 instances at the same time.\n\n\nNOTE\n: Each EFS FileSystem can be accessed only from single VPC at the time.\n\n\nActivating the Driver\n\n\nTo activate the AWS EFS driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nefs\n as the driver name.\n\n\nTroubleshooting\n\n\n\n\nMake sure that AWS credentials (user or role) has following AWS permissions on\n  \nlibStorage\n server instance that will be making calls to AWS API:\n\n\nelasticfilesystem:CreateFileSystem\n\n\nelasticfilesystem:CreateMountTarget\n\n\nec2:DescribeSubnets\n\n\nec2:DescribeNetworkInterfaces\n\n\nec2:CreateNetworkInterface\n\n\nelasticfilesystem:CreateTags\n\n\nelasticfilesystem:DeleteFileSystem\n\n\nelasticfilesystem:DeleteMountTarget\n\n\nec2:DeleteNetworkInterface\n\n\nelasticfilesystem:DescribeFileSystems\n\n\nelasticfilesystem:DescribeMountTargets\n\n\n\n\n\n\n\n\nExamples\n\n\nBelow is a working \nconfig.yml\n file that works with AWS EFS.\n\n\nlibstorage:\n  server:\n    services:\n      efs:\n        driver: efs\n        efs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX\n          securityGroups:\n          - sg-XXXXXXX\n          - sg-XXXXXX0\n          - sg-XXXXXX1\n          region:         us-east-1\n          tag:            test\n\n\n\n\n\n\nSimple Storage Service\n\n\nThe AWS S3FS driver registers a storage driver named \ns3fs\n with the\nlibStorage service registry and provides the ability to mount Amazon Simple\nStorage Service (S3) buckets as filesystems using the\n\ns3fs\n FUSE command.\n\n\nUnlike the other AWS-related drivers, the S3FS storage driver does not need\nto deployed or used by an EC2 instance. Any client can take advantage of\nAmazon's S3 buckets.\n\n\nRequirements\n\n\n\n\nAWS account\n\n\nThe \ns3fs\n FUSE command must be\npresent on client nodes.\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nServer-Side Configuration\n\n\ns3fs:\n  region:           us-east-1\n  accessKey:        XXXXXXXXXX\n  secretKey:        XXXXXXXXXX\n  disablePathStyle: false\n\n\n\n\n\n\nThe \naccessKey\n and \nsecretKey\n configuration parameters are optional and\nshould be used when explicit AWS credentials configuration needs to be provided.\nThe S3FS driver uses the official Golang AWS SDK library and supports all other\nways of  providing access credentials, like environment variables or instance\nprofile IAM permissions.\n\n\nregion\n represents AWS region where S3FS buckets should be provisioned.\nPlease see the official AWS documentation for list of\n\nsupported regions\n.\n\n\nThe \ndisablePathStyle\n property disables the use of the path style for\nbucket endpoints. The path style is more stable with regards to regions\nthan bucket URI FQDNs, but the path style is also less performant.\n\n\n\n\nClient-Side Configuration\n\n\ns3fs:\n  cmd:            s3fs\n  options:\n  - XXXX\n  - XXXX\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n\n\n\n\n\n\nThe \ncmd\n property defaults simply to \ns3fs\n with the assumption that the\n\ns3fs\n binary will be in the path. This value can also be the absolute path\nto the \ns3fs\n binary.\n\n\noptions\n is a list of options to pass to the \ns3fs\n command. Please see the\n\nofficial\n\ndocumentation for a full list of CLI options. The \n-o\n prefix should not be\nprovided in the configuration file.\n\n\nThe credential properties can be defined on the client via the configuration\nfile and will be supplied to the \ns3fs\n process via environment variables.\nHowever, the \ns3fs\n command will also look in all the\n\nusual places\n\nfor the credentials if they're not in this file.\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nRuntime Behavior\n\n\nThe AWS S3FS storage driver can create new buckets as well as remove existing\nones. Buckets are mounted to clients as filesystems using the\n\ns3fs\n FUSE command. For clients\nto correctly mount and unmount S3 buckets the \ns3fs\n command should be in\nthe path of the executor or configured via the \ns3fs.cmd\n property in the\nclient-side REX-Ray configuration file.\n\n\nThe client must also have access to the AWS credentials used for mounting and\nunmounting S3 buckets. These credentials can be stored in the client-side\nREX-Ray configuration file or via\n\nany means avaialble\n\nto the \ns3fs\n command.\n\n\nActivating the Driver\n\n\nTo activate the AWS S3FS driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \ns3fs\n as the driver name.\n\n\nExamples\n\n\nBelow is a working \nconfig.yml\n file that works with AWS S3FS.\n\n\nlibstorage:\n  server:\n    services:\n      s3fs:\n        driver: s3fs\n        s3fs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX\n\n\n\n\nCeph\n\n\nlibStorage includes support for the following Ceph storage technologies.\n\n\n\n\nRADOS Block Device\n\n\nThe Ceph RBD driver registers a driver named \nrbd\n with the \nlibStorage\n driver\nmanager and is used to connect and mount RADOS Block Devices from a Ceph\ncluster.\n\n\nRequirements\n\n\n\n\nThe \nceph\n and \nrbd\n binary executables must be installed on the host\n\n\nThe \nrbd\n kernel module must be installed\n\n\nA \nceph.conf\n file must be present in its default location\n  (\n/etc/ceph/ceph.conf\n)\n\n\nThe ceph \nadmin\n key must be present in \n/etc/ceph/\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured. For a running\nexample see the \nExamples\n section.\n\n\nrbd:\n  defaultPool: rbd\n\n\n\n\nConfiguration Notes\n\n\n\n\nThe \ndefaultPool\n parameter is optional, and defaults to \"rbd\". When set, all\n  volume requests that do not reference a specific pool will use the\n  \ndefaultPool\n value as the destination storage pool.\n\n\n\n\nRuntime behavior\n\n\nThe Ceph RBD driver only works when the client and server are on the same node.\nThere is no way for a centralized \nlibStorage\n server to attach volumes to\nclients, therefore the \nlibStorage\n server must be running on each node that\nwishes to mount RBD volumes.\n\n\nThe RBD driver uses the format of \npool\n.\nname\n for the volume ID. This allows\nfor the use of multiple pools by the driver. During a volume create, if the\nvolume ID is given as \npool\n.\nname\n, a volume named \nname\n will be created in\nthe \npool\n storage pool. If no pool is referenced, the \ndefaultPool\n will be\nused.\n\n\nBoth \npool\n and \nname\n may only contain alphanumeric characters, underscores,\nand dashes.\n\n\nWhen querying volumes, the driver will return all RBDs present in all pools in\nthe cluster, prefixing each volume with the appropriate \npool\n.\n value.\n\n\nAll RBD creates are done using the default 4MB object size, and using the\n\"layering\" feature bit to ensure greatest compatibility with the kernel clients.\n\n\nActivating the Driver\n\n\nTo activate the Ceph RBD driver please follow the instructions for\n\nactivating storage drivers\n, using \nrbd\n as the\ndriver name.\n\n\nTroubleshooting\n\n\n\n\nMake sure that \nceph\n and \nrbd\n commands work without extra parameters for\n  ID, key, and monitors. All configuration must come from \nceph.conf\n.\n\n\nCheck status of the ceph cluster with \nceph -s\n command.\n\n\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n that works with RBD\n\n\nlibstorage:\n  server:\n    services:\n      rbd:\n        driver: rbd\n        rbd:\n          defaultPool: rbd\n\n\n\n\nCaveats\n\n\n\n\nSnapshot and copy functionality is not yet implemented\n\n\nlibStorage Server must be running on each host to mount/attach RBD volumes\n\n\nThere is not yet options for using non-admin cephx keys or changing RBD create\n  features\n\n\nVolume pre-emption is not supported. Ceph does not provide a method to\n  forcefully detach a volume from a remote host -- only a host can attach and\n  detach volumes from itself.\n\n\nRBD advisory locks are not yet in use. A volume is returned as \"unavailable\"\n  if it has a watcher other than the requesting client. Until advisory locks are\n  in place, it may be possible for a client to attach a volume that is already\n  attached to another node. Mounting and writing to such a volume could lead to\n  data corruption.\n\n\n\n\nDell EMC\n\n\nlibStorage includes support for several Dell EMC storage platforms.\n\n\n\n\nIsilon\n\n\nThe Isilon driver registers a storage driver named \nisilon\n with the\nlibStorage service registry and is used to connect and manage Isilon NAS\nstorage. The driver creates logical volumes in directories on the Isilon\ncluster. Volumes are exported via NFS and restricted to a single client at a\ntime. Quotas can also be used to ensure that a volume directory doesn't exceed\na specified size.\n\n\nConfiguration\n\n\nThe following is an example configuration of the Isilon driver.\n\n\nisilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  group: groupname\n  password: password\n  volumePath: /libstorage\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how configuration properties are\n\ntransformed\n.\n\n\nExtra Parameters\n\n\nThe following items are configurable specific to this driver.\n\n\n\n\nvolumePath\n represents the location under \n/ifs/volumes\n to allow volumes to\n   be created and removed.\n\n\nnfsHost\n is the configurable NFS server hostname or IP (often a\n   SmartConnect name) used when mounting exports\n\n\ndataSubnet\n is the subnet the REX-Ray driver is running on. This is used\n   for the NFS export host ACLs.\n\n\n\n\nOptional Parameters\n\n\nThe following items are not required, but available to this driver.\n\n\n\n\ninsecure\n defaults to \nfalse\n.\n\n\ngroup\n defaults to the group of the user specified in the configuration.\n   Only use this option if you need volumes to be created with a different\n   group.\n\n\nvolumePath\n defaults to \"\". This will have all new volumes created directly\n   under \n/ifs/volumes\n.\n\n\nquotas\n defaults to \nfalse\n. Set to \ntrue\n if you have a SmartQuotas\n   license enabled.\n\n\n\n\nActivating the Driver\n\n\nTo activate the Isilon driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nisilon\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with Isilon.\n\n\nlibstorage:\n  server:\n    services:\n      isilon:\n        driver: isilon\n        isilon:\n          endpoint: https://endpoint:8080\n          insecure: true\n          username: username\n          password: password\n          volumePath: /libstorage\n          nfsHost: nfsHost\n          dataSubnet: subnet\n          quotas: true\n\n\n\n\nInstructions\n\n\nIt is expected that the \nvolumePath\n exists already within the Isilon system.\nThis example would reflect a directory create under \n/ifs/volumes/libstorage\n\nfor created volumes. It is not necessary to export this volume. The \ndataSubnet\n\nparameter is required so the Isilon driver can restrict access to attached\nvolumes to the host that REX-Ray is running on.\n\n\nIf \nquotas\n are enabled, a SmartQuotas license must also be enabled on the\nIsilon cluster for the capacity size functionality of \nlibStorage\n to work.\n\n\nA SnapshotIQ license must be enabled on the Isilon cluster for the snapshot\nfunctionality of \nlibStorage\n to work.\n\n\nCaveats\n\n\nThe Isilon driver is not without its caveats:\n\n\n\n\nThe account used to access the Isilon cluster must be in a role with the\n  following privileges:\n\n\nNamespace Access (ISI_PRIV_NS_IFS_ACCESS)\n\n\nPlatform API (ISI_PRIV_LOGIN_PAPI)\n\n\nNFS (ISI_PRIV_NFS)\n\n\nRestore (ISI_PRIV_IFS_RESTORE)\n\n\nQuota (ISI_PRIV_QUOTA)          (if \nquotas\n are enabled)\n\n\nSnapshot (ISI_PRIV_SNAPSHOT)    (if snapshots are used)\n\n\n\n\n\n\n\n\n\n\nScaleIO\n\n\nThe ScaleIO driver registers a storage driver named \nscaleio\n with the\nlibStorage service registry and is used to connect and manage ScaleIO storage.\n\n\nRequirements\n\n\n\n\nThe ScaleIO \nREST Gateway\n is required for the driver to function.\n\n\nThe \nlibStorage\n client or application that embeds the \nlibStorage\n client\n   must reside on a host that has the SDC client installed. The command\n   \n/opt/emc/scaleio/sdc/bin/drv_cfg --query_guid\n should be executable and\n   should return the local SDC GUID.\n\n\nThe \nofficial\n\n   Oracle Java Runtime Environment (JRE) is required. During testing, use of the\n   Open Java Development Kit (JDK) resulted in unexpected errors.\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nscaleio:\n  endpoint:             https://host_ip/api\n  apiVersion:           \n2.0\n\n  insecure:             false\n  useCerts:             true\n  userName:             admin\n  password:             mypassword\n  systemID:             0\n  systemName:           sysv\n  protectionDomainID:   0\n  protectionDomainName: corp\n  storagePoolID:        0\n  storagePoolName:      gold\n  thinOrThick:          ThinProvisioned\n\n\n\n\nConfiguration Notes\n\n\n\n\nThe \napiVersion\n can optionally be set here to force certain API behavior.\nThe default is to retrieve the endpoint API, and pass this version during calls.\n\n\ninsecure\n should be set to \ntrue\n if you have not loaded the SSL\ncertificates on the host.  A successful wget or curl should be possible without\nSSL errors to the API \nendpoint\n in this case.\n\n\nuseCerts\n should only be set if you want to leverage the internal SSL\ncertificates.  This would be useful if you are deploying the REX-Ray binary\non a host that does not have any certificates installed.\n\n\nsystemID\n takes priority over \nsystemName\n.\n\n\nprotectionDomainID\n takes priority over \nprotectionDomainName\n.\n\n\nstoragePoolID\n takes priority over \nstoragePoolName\n.\n\n\nthinkOrThick\n determines whether to provision as the default\n\nThinProvisioned\n, or \nThickProvisioned\n.\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nRuntime Behavior\n\n\nThe \nstorageType\n field that is configured per volume is considered the\nScaleIO Storage Pool.  This can be configured by default with the \nstoragePool\n\nsetting.  It is important that you create unique names for your Storage Pools\non the same ScaleIO platform.  Otherwise, when specifying \nstorageType\n it\nmay choose at random which \nprotectionDomain\n the pool comes from.\n\n\nThe \navailabilityZone\n field represents the ScaleIO Protection Domain.\n\n\nConfiguring the Gateway\n\n\n\n\nInstall the \nEMC-ScaleIO-gateway\n package.\n\n\nEdit the\n\n/opt/emc/scaleio/gateway/webapps/ROOT/WEB-INF/classes/gatewayUser.properties\n\nfile and append the proper MDM IP addresses to the following \nmdm.ip.addresses=\n\nparameter.\n\n\nBy default the password is the same as your administrative MDM password.\n\n\nStart the gateway \nservice scaleio-gateway start\n.\n\n\nWith 1.32 we have noticed a restart of the gateway may be necessary as well\nafter an initial install with \nservice scaleio-gateway restart\n.\n\n\n\n\nActivating the Driver\n\n\nTo activate the ScaleIO driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nscaleio\n as the driver name.\n\n\nTroubleshooting\n\n\n\n\nVerify your parameters for \nsystem\n, \nprotectionDomain\n, and\n\nstoragePool\n are correct.\n\n\nVerify that have the ScaleIO SDC service installed with\n\nrpm -qa EMC-ScaleIO-sdc\n\n\nVerify that the following command returns the local SDC GUID\n\n/opt/emc/scaleio/sdc/bin/drv_cfg --query_guid\n.\n\n\nEnsure that you are able to open a TCP connection to the gateway with the\naddress that you will be supplying below in the \ngateway_ip\n parameter.  For\nexample \ntelnet gateway_ip 443\n should open a successful connection.  Removing\nthe \nEMC-ScaleIO-gateway\n package and reinstalling can force re-creation of\nself-signed certs which may help resolve gateway problems.  Also try restarting\nthe gateway with \nservice scaleio-gateway restart\n.\n\n\nEnsure that you have the correct authentication credentials for the gateway.\nThis can be done with a curl login. You should receive an authentication\ntoken in return.\n\ncurl --insecure --user admin:XScaleio123 https://gw_ip:443/api/login\n\n\nPlease review the gateway log at\n\n/opt/emc/scaleio/gateway/logs/catalina.out\n for errors.\n\n\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with ScaleIO.\n\n\nlibstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n        scaleio:\n          endpoint: https://gateway_ip/api\n          insecure: true\n          userName: username\n          password: password\n          systemName: tenantName\n          protectionDomainName: protectionDomainName\n          storagePoolName: storagePoolName\n\n\n\n\nDigitalOcean\n\n\nThanks to the efforts of our tremendous community, libStorage also has built-in\nsupport for DigitalOcean!\n\n\n\n\n\n\nDO Block Storage\n\n\nThe DigitalOcean Block Storage (DOBS) driver registers a driver named \ndobs\n\nwith the libStorage service registry and is used to attach and mount\nDigitalOcean block storage devices to DigitalOcean instances.\n\n\n\n\nNote\n\n\nThe DigitalOcean Block Storage driver currently only supports operating in\n\nlocal only\n mode where the libStorage server must be running on the same\nhost as the client.\n\n\n\n\nRequirements\n\n\nThe DigitalOcean block storage driver has the following requirements:\n\n\n\n\nValid DigitalOcean account\n\n\nValid DigitalOcean \naccess token\n\n\n\n\nConfiguration\n\n\nThe following example illustrates a minimal configuration for the DigitalOcean\nblock storage driver:\n\n\ndobs:\n  token:  123456\n  region: nyc1\n\n\n\n\n\n\nNote\n\n\nThe standard environment variable for the DigitalOcean access token is\n\nDIGITALOCEAN_ACCESS_TOKEN\n. However, the environment variable mapped to\nthis driver's \ndobs.token\n property is \nDOBS_TOKEN\n. This choice was made\nto ensure that the driver must be explicitly configured for access instead\nof detecting a default token that may not be intended for the driver.\n\n\n\n\nFittedCloud\n\n\nAnother example of the great community shared by the libStorage project, the\ntalented people at FittedCloud have provided a driver for their EBS optimizer.\n\n\n\n\n\n\nEBS Optimizer\n\n\nThe FittedCloud EBS Optimizer driver registers a storage driver named\n\nfittedcloud\n with the libStorage service registry and provides the ability to\nconnect and manage thin-provisioned EBS volumes for EC2 instances.\n\n\n\n\nNote\n\n\nThis version of the FittedCloud driver only supports configurations where\nclient and server are on the same host.  The libStorage server must be\nrunning on each node along side with the FittedCloud Agent.\n\n\n\n\n\n\nNote\n\n\nThis version of the FittedCloud driver does not support co-existing with the\nebs driver on the same host. As a result it also doesn't support optimizing\nexisting EBS volumes.   See the \nExamples\n section\nbelow for a running example.\n\n\n\n\n\n\nNote\n\n\nThe FittedCloud driver does not yet support snapshots or tags.\n\n\n\n\nRequirements\n\n\nThis driver has the following requirements:\n\n\n\n\nAWS account\n\n\nVPC - EBS can be accessed within VPC\n\n\nAWS Credentials\n\n\nFittedCloud Agent software\n\n\n\n\n\n\nGetting Started\n\n\nBefore starting, please make sure to register as a user by visiting the\nFittedCloud \ncustomer website\n.\nOnce an account is activated it will be assigned a user ID, which can be found\non the Settings page after logging into the web site.\n\n\nThe following commands will download and install the latest FittedCloud Agent\nsoftware. The flags \n-o S -m\n enable new thin volumes to be created via the\ndocker command instead of optimizing existing EBS volumes.\nPlease replace the \nUser ID\n with a FittedCloud user ID.\n\n\n$ curl -skSL 'https://customer.fittedcloud.com/downloadsoftware?ver=latest' \\\n  -o fcagent.run\n$ sudo bash ./fcagent.run -- -o S -m -d \nUser ID\n\n\n\n\n\nPlease refer to FittedCloud\n\nwebsite\n for more details.\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nebs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  kmsKeyID:       abcd1234-a123-456a-a12b-a123b4cd56ef\n\n\n\n\nConfiguration Notes\n\n\n\n\nFittedCloud driver shares the ebs driver's configuration\nparameters.\n\n\nThe \naccessKey\n and \nsecretKey\n configuration parameters are optional and\nshould be used when explicit AWS credentials configuration needs to be provided.\nFittedCloud driver uses official golang AWS SDK library and supports all other\nways of providing access credentials, like environment variables or instance\nprofile IAM permissions.\n\n\nIf the \nkmsKeyID\n field is specified it will be used as the encryption key for\nall volumes that are created with a truthy encryption request field.\n\n\n\n\n\n\nExamples\n\n\nThe following example illustrates how to configured the FittedCloud driver:\n\n\nlibstorage:\n  service:    fittedcloud\nebs:\n  accessKey:  XXXXXXXXXX\n  secretKey:  XXXXXXXXXX\n\n\n\n\nAdditional information on configuring the FittedCloud driver may be found\nat \nthis\n location.\n\n\nGoogle\n\n\nlibStorage ships with support for Google Compute Engine (GCE) as well.\n\n\n\n\nGCE Persistent Disk\n\n\nThe Google Compute Engine Persistent Disk (GCEPD) driver registers a driver\nnamed \ngcepd\n with the libStorage service registry and is used to connect and\nmount Google Compute Engine (GCE) persistent disks with GCE machine instances.\n\n\nRequirements\n\n\n\n\nGCE account\n\n\nThe libStorage server must be running on a GCE instance created with a Service\n  Account with appropriate permissions, or a Service Account credentials file\n  in JSON format must be supplied. If not using the Compute Engine default\n  Service Account with the Cloud Platform/\"all cloud APIs\" scope, create a new\n  Service Account via the \nIAM Portal\n.\n  This Service Account requires the \nCompute Engine/Instance Admin\n,\n  \nCompute Engine/Storage Admin\n, and \nProject/Service Account Actor\n roles.\n  Then create/download a new private key in JSON format. see\n  \ncreating a service account\n\n  for details. The libStorage service must be restarted in order for permissions\n  changes on a service account to take effect.\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured. For a running\nexample see the \nExamples\n section.\n\n\ngcepd:\n  keyfile: /etc/gcekey.json\n  zone: us-west1-b\n  defaultDiskType: pd-ssd\n  tag: rexray\n\n\n\n\nConfiguration Notes\n\n\n\n\nThe \nkeyfile\n parameter is optional. It specifies a path on disk to a file\n  containing the JSON-encoded Service Account credentials. This file can be\n  downloaded from the GCE web portal. If \nkeyfile\n is specified, the GCE\n  instance's service account is not considered, and is not necessary. If\n  \nkeyfile\n is \nnot\n specified, the application will try to lookup\n  \napplication default credentials\n.\n  This has the effect of looking for credentials in the priority described\n  \nhere\n.\n\n\nThe \nzone\n parameter is optional, and configures the driver to \nonly\n allow\n  access to the given zone. Creating and listing disks from other zones will be\n  denied. If a zone is not specified, the zone from the client Instance ID will\n  be used when creating new disks.\n\n\nThe \ndefaultDiskType\n parameter is optional and specifies what type of disk\n  to create, either \npd-standard\n or \npd-ssd\n. When not specified, the default\n  is \npd-ssd\n.\n\n\nThe \ntag\n parameter is optional, and causes the driver to create or return\n  disks that have a matching tag. The tag is implemented by using the GCE\n  label functionality available in the beta API. The value of the \ntag\n\n  parameter is used as the value for a label with the key \nlibstoragetag\n.\n  Use of this parameter is encouraged, as the driver will only return volumes\n  that have been created by the driver, which is most useful to eliminate\n  listing the boot disks of every GCE disk in your project/zone. If you wsih to\n  \"expose\" previously created disks to the \nGCEPD\n driver, you can edit the\n  labels on the existing disk to have a key of \nlibstoragetag\n and a value\n  matching that given in \ntag\n.\n\n\n\n\nRuntime behavior\n\n\n\n\nThe GCEPD driver enforces the GCE requirements for disk sizing and naming.\n  Disks must be created with a minimum size of 10GB. Disk names must adhere to\n  the regular expression of \n[a-z]([-a-z0-9]*[a-z0-9])?\n, which means the first\n  character must be a lowercase letter, and all following characters must be a\n  dash, lowercase letter, or digit, except the last character, which cannot be a\n  dash.\n\n\nIf the \nzone\n parameter is not specified in the driver configuration, and a\n  request is received to list all volumes that does not specify a zone in the\n  InstanceID header, volumes from all zones will be returned.\n\n\nBy default, all disks will be created with type \npd-ssd\n, which creates an SSD\n  based disk. If you wish to create disks that are not SSD-based, change the\n  default via the driver config, or the type can be changed at creation time by\n  using the \nType\n field of the create request.\n\n\n\n\nActivating the Driver\n\n\nTo activate the GCEPD driver please follow the instructions for\n\nactivating storage drivers\n, using \ngcepd\n as the\ndriver name.\n\n\nTroubleshooting\n\n\n\n\nMake sure that the JSON credentials file as specified in the \nkeyfile\n\n  configuration parameter is present and accessible, or that you are running in\n  a GCE instance created with a Service Account attached. Whether using a\n  \nkeyfile\n or the Service Account associated with the GCE instance, the Service\n  Account must have the appropriate permissions as described in\n  \nConfiguration Notes\n\n\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n that works with GCE\n\n\nlibstorage:\n  server:\n    services:\n      gcepd:\n        driver: gcepd\n        gcepd:\n          keyfile: /etc/gcekey.json\n          tag: rexray\n\n\n\n\nCaveats\n\n\n\n\nSnapshot and copy functionality is not yet implemented\n\n\nMost GCE instances can have up to 64 TB of total persistent disk space\n  attached. Shared-core machine types or custom machine types with less than\n  3.75 GB of memory are limited to 3 TB of total persistent disk space. Total\n  persistent disk space for an instance includes the size of the root persistent\n  disk. You can attach up to 16 independent persistent disks to most instances,\n  but instances with shared-core machine types or custom machine types with less\n  than 3.75 GB of memory are limited to a maximum of 4 persistent disks,\n  including the root persistent disk. See\n  \nGCE Disks\n docs for more\n  details.\n\n\nIf running libStorage server in a mode where volume mounts will not be\n  performed on the same host where libStorage server is running, it should be\n  possible to use a Service Account without the \nService Account Actor\n role, but\n  this has not been tested. Note that if persistent disk mounts are to be\n  performed on \nany\n GCE instances that have a Service Account associated with\n  the, the \nService Account Actor\n role is required.\n\n\n\n\nMicrosoft\n\n\nMicrosoft Azure support is included with libStorage as well.\n\n\n\n\nAzure Unmanaged Disk\n\n\nThe Microsoft Azure Unmanaged Disk (Azure UD) driver registers a driver\nnamed \nazureud\n with the libStorage service registry and is used to connect and\nmount Azure unmanaged disks from Azure page blob storage with Azure virtual\nmachines.\n\n\nRequirements\n\n\n\n\nAn Azure account\n\n\nAn Azure subscription\n\n\nAn Azure storage account\n\n\nAn Azure resource group\n\n\nAny virtual machine where disks are going to be attached must have the\n  \nlsscsi\n utility installed. You can install this with \nyum install lsscsi\n on\n  Red Hat based distributions, or with \napt-get install lsscsi\n on Debian based\n  distributions.\n\n\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured. For a running\nexample see the \nExamples\n section.\n\n\nazureud:\n  subscriptionID: abcdef01-2345-6789-abcd-ef0123456789\n  resourceGroup: testgroup\n  tenantID: usernamehotmail.onmicrosoft.com\n  storageAccount: username\n  storageAccessKey: XXXXXXXX\n  clientID: 123def01-2345-6789-abcd-ef0123456789\n  clientSecret: XXXXXXXX\n  certPath:\n  container: vhds\n  useHTTPS: true\n\n\n\n\nConfiguration Notes\n\n\n\n\nsubscriptionID\n is required, and is the UUID of your Azure subscription\n\n\nresourceGroup\n is required, and is the name of the resource group for your\n  VMs and storage.\n\n\ntenantID\n is required, and is either the domain or UUID for your active\n  directory account within Azure.\n\n\nstorageAccount\n is required, and is the name of the storage account where\n  your disks will be created.\n\n\nstorageAccessKey\n is required, and is a valid access key associated with the\n  \nstorageAccount\n.\n\n\nclientID\n is required, and is the UUID of your client, which was created as\n  an App Registration within your Azure active directory account.\n\n\nclientSecret\n is required if \ncertPath\n is not provided instead. It is a\n  valid access key associated with \nclientID\n, and is managed as part of the App\n  Registration.\n\n\ncertPath\n is an alternative to \nclientSecret\n, contains the location of a\n  PKCS encoded RSA private key associated with \nclientID\n.\n\n\ncontainer\n is optional, and specifies the name of an existing container\n  within \nstorageAccount\n. This container must already exist and is not created\n  automatically.\n\n\nuseHTTPS\n is optional, and is a boolean value on whether to use HTTPS when\n  communicating with the Azure storage endpoint.\n\n\n\n\nRuntime Behavior\n\n\n\n\nThe \ncontainer\n config option defaults to \nvhds\n, and this container is\n  present by default in Azure. Changing this option is only necessary if you\n  want to use a different container within your storage account.\n\n\nVolume Attach/Detach operations in Azure take a long time, sometimes greater\n  than 60 seconds, which is libStorage's default task timeout. When the timeout\n  is hit, libStorage returns information to the caller about a queued task, and\n  that the task is still running. This may cause issues for upstream callers.\n  It is \nhighly\n recommended to adjust this default timeout to 120 seconds by\n  setting the \nlibstorage.server.tasks.exeTimeout\n property. This is done in\n  the \nExamples\n section below.\n\n\n\n\nActivating the Driver\n\n\nTo activate the Azure UD driver please follow the instructions for\n\nactivating storage drivers\n, using \nazureud\n as\nthe driver name.\n\n\nTroubleshooting\n\n\n\n\nAfter creating your app registration, you must go into the\n  \nRequired Permissions\n tab and grant access to \"Windows Azure Service\n  Management API\". Choose the delegated permission for accessing as organization\n  users.\n\n\nYou must also grant your app registration access to your subscription, by\n  going to Subscriptions-\nYour \nsubscriptionID\n-\nAccess Control (IAM). From\n  there, add your app registration as a user, which you will have to search for\n  by name. Grant the role of \"Owner\".\n\n\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n that works with Azure UD\n\n\nlibstorage:\n  server:\n    tasks:\n      exeTimeout: 120s\n    services:\n      azure:\n        driver: azureud\n        azureud:\n          subscriptionID: abcdef01-2345-6789-abcd-ef0123456789\n          resourceGroup: testgroup\n          tenantID: usernamehotmail.onmicrosoft.com\n          storageAccount: username\n          storageAccessKey: XXXXXXXX\n          clientID: 123def01-2345-6789-abcd-ef0123456789\n          clientSecret: XXXXXXXX\n\n\n\n\nCaveats\n\n\n\n\nSnapshot and Copy functionality is not yet implemented\n\n\nThe number of disks you can attach to a Virtual Machine depends on its type.\n\n\nGood resources for reading about disks in Azure are\n  \nhere\n\n  and \nhere\n.\n\n\n\n\nVirtualBox\n\n\nThe VirtualBox driver registers a storage driver named \nvirtualbox\n with the\nlibStorage service registry and is used by VirtualBox's VMs to connect and\nmanage volumes provided by VirtualBox.\n\n\nPrerequisites\n\n\nIn order to leverage the \nvirtualbox\n driver, the \nlibStorage\n client or must\nbe located on each VM that you wish to be able to consume external volumes.\nThe driver leverages the \nvboxwebserv\n HTTP SOAP API which is a process that\nmust be started from the VirtualBox \nhost\n (ie OS X) using\n\nvboxwebsrv -H 0.0.0.0 -v\n or additionally with \n-b\n for running in the\nbackground. This allows the VMs running \nlibStorage\n to remotely make calls to\nthe underlying VirtualBox application. A test for connectivity can be done with\n\ntelnet virtualboxip 18083\n from the VM. The \nvirtualboxip\n is what you\nwould put in the \nendpoint\n value.\n\n\nLeveraging authentication for the VirtualBox webserver is optiona.. The HTTP\nSOAP API can have authentication disabled by running\n\nVBoxManage setproperty websrvauthlibrary null\n.\n\n\nHot-Plugging is required, which limits the usefulness of this driver to \nSATA\n\nonly.  Ensure that your VM has \npre-created\n this controller and it is\nnamed \nSATA\n.  Otherwise the \ncontrollerName\n field must be populated\nwith the name of the controller you wish to use.  The port count must be set\nmanually as it cannot be increased when the VMs are on.  A count of \n30\n\nis suggested.\n\n\nVirtualBox 5.0.10+ must be used.\n\n\nConfiguration\n\n\nThe following is an example configuration of the VirtualBox driver.\n\nThe \nlocalMachineNameOrId\n parameter is for development use where you force\n\nlibStorage\n to use a specific VM identity.  Choose a \nvolumePath\n to store the\nvolume files or virtual disks.  This path should be created ahead of time.\n\n\nvirtualbox:\n  endpoint: http://virtualboxhost:18083\n  userName: optional\n  password: optional\n  tls: false\n  volumePath: $HOME/VirtualBox/Volumes\n  controllerName: name\n  localMachineNameOrId: forDevelopmentUse\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the VirtualBox driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nvirtualbox\n as the driver name.\n\n\nExamples\n\n\nBelow is a working \nconfig.yml\n file that works with VirtualBox.\n\n\nlibstorage:\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nCaveats\n\n\n\n\nSnapshot and create volume from volume functionality is not available yet\n  with this driver.\n\n\nThe driver supports VirtualBox 5.0.10+", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#storage-providers", 
            "text": "Connecting storage and platforms...", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#overview", 
            "text": "This page reviews the storage providers and platforms supported by  libStorage .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/storage-providers/#clientserver-configuration", 
            "text": "Regarding the examples below, please read the provision  about\nclient/server configurations before proceeding.", 
            "title": "Client/Server Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#amazon", 
            "text": "libStorage includes support for multiple Amazon Web Services (AWS) storage\nservices.", 
            "title": "Amazon"
        }, 
        {
            "location": "/user-guide/storage-providers/#elastic-block-storage", 
            "text": "The AWS EBS driver registers a storage driver named  ebs  with the\nlibStorage service registry and is used to connect and manage AWS Elastic Block\nStorage volumes for EC2 instances.   Note  For backwards compatibility, the driver also registers a storage driver\nnamed  ec2 . The use of  ec2  in config files is deprecated but functional.    Note  The EBS driver does not yet support snapshots or tags, as previously supported\nin Rex-Ray v0.3.3.   The EBS driver is made possible by the official Amazon Go AWS SDK .", 
            "title": "Elastic Block Storage"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements", 
            "text": "AWS account  VPC - EBS can be accessed within VPC  AWS Credentials", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.  ebs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  region:         us-east-1\n  kmsKeyID:       arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes", 
            "text": "The  accessKey  and  secretKey  configuration parameters are optional and should\nbe used when explicit AWS credentials configuration needs to be provided. EBS driver\nuses official golang AWS SDK library and supports all other ways of providing\naccess credentials, like environment variables or instance profile IAM permissions.  region  represents AWS region where EBS volumes should be provisioned.\nSee official AWS documentation for list of supported regions.  If the  kmsKeyID  field is specified it will be used as the encryption key for\nall volumes that are created with a truthy encryption request field.   For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver", 
            "text": "To activate the AWS EBS driver please follow the instructions for activating storage drivers ,\nusing  ebs  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting", 
            "text": "Make sure that AWS credentials (user or role) has following AWS permissions on\n   libStorage  server instance that will be making calls to AWS API:  ec2:AttachVolume ,  ec2:CreateVolume ,  ec2:CreateSnapshot ,  ec2:CreateTags ,  ec2:DeleteVolume ,  ec2:DeleteSnapshot ,  ec2:DescribeAvailabilityZones ,  ec2:DescribeInstances ,  ec2:DescribeVolumes ,  ec2:DescribeVolumeAttribute ,  ec2:DescribeVolumeStatus ,  ec2:DescribeSnapshots ,  ec2:CopySnapshot ,  ec2:DescribeSnapshotAttribute ,  ec2:DetachVolume ,  ec2:ModifySnapshotAttribute ,  ec2:ModifyVolumeAttribute ,  ec2:DescribeTags", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples", 
            "text": "Below is a working  config.yml  file that works with AWS EBS.  libstorage:\n  server:\n    services:\n      ebs:\n        driver: ebs\n        ebs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX\n          region:         us-east-1", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#elastic-file-system", 
            "text": "The AWS EFS driver registers a storage driver named  efs  with the\nlibStorage service registry and is used to connect and manage AWS Elastic File\nSystems.", 
            "title": "Elastic File System"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_1", 
            "text": "AWS account  VPC - EFS can be accessed within VPC  AWS Credentials", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_1", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.  efs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  securityGroups:\n  - sg-XXXXXXX\n  - sg-XXXXXX0\n  - sg-XXXXXX1\n  region:              us-east-1\n  tag:                 test\n  disableSessionCache: false", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_1", 
            "text": "The  accessKey  and  secretKey  configuration parameters are optional and should\nbe used when explicit AWS credentials configuration needs to be provided. EFS driver\nuses official golang AWS SDK library and supports all other ways of providing\naccess credentials, like environment variables or instance profile IAM permissions.  region  represents AWS region where EFS should be provisioned. See official AWS\ndocumentation for list of supported regions.  securityGroups  list of security groups attached to  MountPoint  instances.\nIf no security groups are provided the default VPC security group is used.  tag  is used to partition multiple services within single AWS account and is\nused as prefix for EFS names in format  [tagprefix]/volumeName .  disableSessionCache  is a flag that can be used to disable the session cache.\nIf the session cache is disabled then a new AWS connection is established with\nevery API call.   For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior", 
            "text": "AWS EFS storage driver creates one EFS FileSystem per volume and provides root\nof the filesystem as NFS mount point. Volumes aren't attached to instances\ndirectly but rather exposed to each subnet by creating  MountPoint  in each VPC\nsubnet. When detaching volume from instance no action is taken as there isn't\ngood way to figure out if there are other instances in same subnet using MountPoint  that is being detached. There is no charge for  MountPoint \nso they are removed only once whole volume is deleted.  By default all EFS instances are provisioned as  generalPurpose  performance mode. maxIO  EFS type can be provisioned by providing  maxIO  flag as  volumetype .  Its possible to mount same volume to multiple container on a single EC2 instance\nas well as use single volume across multiple EC2 instances at the same time.  NOTE : Each EFS FileSystem can be accessed only from single VPC at the time.", 
            "title": "Runtime Behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_1", 
            "text": "To activate the AWS EFS driver please follow the instructions for activating storage drivers ,\nusing  efs  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting_1", 
            "text": "Make sure that AWS credentials (user or role) has following AWS permissions on\n   libStorage  server instance that will be making calls to AWS API:  elasticfilesystem:CreateFileSystem  elasticfilesystem:CreateMountTarget  ec2:DescribeSubnets  ec2:DescribeNetworkInterfaces  ec2:CreateNetworkInterface  elasticfilesystem:CreateTags  elasticfilesystem:DeleteFileSystem  elasticfilesystem:DeleteMountTarget  ec2:DeleteNetworkInterface  elasticfilesystem:DescribeFileSystems  elasticfilesystem:DescribeMountTargets", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_1", 
            "text": "Below is a working  config.yml  file that works with AWS EFS.  libstorage:\n  server:\n    services:\n      efs:\n        driver: efs\n        efs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX\n          securityGroups:\n          - sg-XXXXXXX\n          - sg-XXXXXX0\n          - sg-XXXXXX1\n          region:         us-east-1\n          tag:            test", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#simple-storage-service", 
            "text": "The AWS S3FS driver registers a storage driver named  s3fs  with the\nlibStorage service registry and provides the ability to mount Amazon Simple\nStorage Service (S3) buckets as filesystems using the s3fs  FUSE command.  Unlike the other AWS-related drivers, the S3FS storage driver does not need\nto deployed or used by an EC2 instance. Any client can take advantage of\nAmazon's S3 buckets.", 
            "title": "Simple Storage Service"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_2", 
            "text": "AWS account  The  s3fs  FUSE command must be\npresent on client nodes.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_2", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#server-side-configuration", 
            "text": "s3fs:\n  region:           us-east-1\n  accessKey:        XXXXXXXXXX\n  secretKey:        XXXXXXXXXX\n  disablePathStyle: false   The  accessKey  and  secretKey  configuration parameters are optional and\nshould be used when explicit AWS credentials configuration needs to be provided.\nThe S3FS driver uses the official Golang AWS SDK library and supports all other\nways of  providing access credentials, like environment variables or instance\nprofile IAM permissions.  region  represents AWS region where S3FS buckets should be provisioned.\nPlease see the official AWS documentation for list of supported regions .  The  disablePathStyle  property disables the use of the path style for\nbucket endpoints. The path style is more stable with regards to regions\nthan bucket URI FQDNs, but the path style is also less performant.", 
            "title": "Server-Side Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#client-side-configuration", 
            "text": "s3fs:\n  cmd:            s3fs\n  options:\n  - XXXX\n  - XXXX\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX   The  cmd  property defaults simply to  s3fs  with the assumption that the s3fs  binary will be in the path. This value can also be the absolute path\nto the  s3fs  binary.  options  is a list of options to pass to the  s3fs  command. Please see the official \ndocumentation for a full list of CLI options. The  -o  prefix should not be\nprovided in the configuration file.  The credential properties can be defined on the client via the configuration\nfile and will be supplied to the  s3fs  process via environment variables.\nHowever, the  s3fs  command will also look in all the usual places \nfor the credentials if they're not in this file.   For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Client-Side Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior_1", 
            "text": "The AWS S3FS storage driver can create new buckets as well as remove existing\nones. Buckets are mounted to clients as filesystems using the s3fs  FUSE command. For clients\nto correctly mount and unmount S3 buckets the  s3fs  command should be in\nthe path of the executor or configured via the  s3fs.cmd  property in the\nclient-side REX-Ray configuration file.  The client must also have access to the AWS credentials used for mounting and\nunmounting S3 buckets. These credentials can be stored in the client-side\nREX-Ray configuration file or via any means avaialble \nto the  s3fs  command.", 
            "title": "Runtime Behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_2", 
            "text": "To activate the AWS S3FS driver please follow the instructions for activating storage drivers ,\nusing  s3fs  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_2", 
            "text": "Below is a working  config.yml  file that works with AWS S3FS.  libstorage:\n  server:\n    services:\n      s3fs:\n        driver: s3fs\n        s3fs:\n          accessKey:      XXXXXXXXXX\n          secretKey:      XXXXXXXXXX", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#ceph", 
            "text": "libStorage includes support for the following Ceph storage technologies.", 
            "title": "Ceph"
        }, 
        {
            "location": "/user-guide/storage-providers/#rados-block-device", 
            "text": "The Ceph RBD driver registers a driver named  rbd  with the  libStorage  driver\nmanager and is used to connect and mount RADOS Block Devices from a Ceph\ncluster.", 
            "title": "RADOS Block Device"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_3", 
            "text": "The  ceph  and  rbd  binary executables must be installed on the host  The  rbd  kernel module must be installed  A  ceph.conf  file must be present in its default location\n  ( /etc/ceph/ceph.conf )  The ceph  admin  key must be present in  /etc/ceph/", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_3", 
            "text": "The following is an example with all possible fields configured. For a running\nexample see the  Examples  section.  rbd:\n  defaultPool: rbd", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_2", 
            "text": "The  defaultPool  parameter is optional, and defaults to \"rbd\". When set, all\n  volume requests that do not reference a specific pool will use the\n   defaultPool  value as the destination storage pool.", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior_2", 
            "text": "The Ceph RBD driver only works when the client and server are on the same node.\nThere is no way for a centralized  libStorage  server to attach volumes to\nclients, therefore the  libStorage  server must be running on each node that\nwishes to mount RBD volumes.  The RBD driver uses the format of  pool . name  for the volume ID. This allows\nfor the use of multiple pools by the driver. During a volume create, if the\nvolume ID is given as  pool . name , a volume named  name  will be created in\nthe  pool  storage pool. If no pool is referenced, the  defaultPool  will be\nused.  Both  pool  and  name  may only contain alphanumeric characters, underscores,\nand dashes.  When querying volumes, the driver will return all RBDs present in all pools in\nthe cluster, prefixing each volume with the appropriate  pool .  value.  All RBD creates are done using the default 4MB object size, and using the\n\"layering\" feature bit to ensure greatest compatibility with the kernel clients.", 
            "title": "Runtime behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_3", 
            "text": "To activate the Ceph RBD driver please follow the instructions for activating storage drivers , using  rbd  as the\ndriver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting_2", 
            "text": "Make sure that  ceph  and  rbd  commands work without extra parameters for\n  ID, key, and monitors. All configuration must come from  ceph.conf .  Check status of the ceph cluster with  ceph -s  command.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_3", 
            "text": "Below is a full  config.yml  that works with RBD  libstorage:\n  server:\n    services:\n      rbd:\n        driver: rbd\n        rbd:\n          defaultPool: rbd", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats", 
            "text": "Snapshot and copy functionality is not yet implemented  libStorage Server must be running on each host to mount/attach RBD volumes  There is not yet options for using non-admin cephx keys or changing RBD create\n  features  Volume pre-emption is not supported. Ceph does not provide a method to\n  forcefully detach a volume from a remote host -- only a host can attach and\n  detach volumes from itself.  RBD advisory locks are not yet in use. A volume is returned as \"unavailable\"\n  if it has a watcher other than the requesting client. Until advisory locks are\n  in place, it may be possible for a client to attach a volume that is already\n  attached to another node. Mounting and writing to such a volume could lead to\n  data corruption.", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#dell-emc", 
            "text": "libStorage includes support for several Dell EMC storage platforms.", 
            "title": "Dell EMC"
        }, 
        {
            "location": "/user-guide/storage-providers/#isilon", 
            "text": "The Isilon driver registers a storage driver named  isilon  with the\nlibStorage service registry and is used to connect and manage Isilon NAS\nstorage. The driver creates logical volumes in directories on the Isilon\ncluster. Volumes are exported via NFS and restricted to a single client at a\ntime. Quotas can also be used to ensure that a volume directory doesn't exceed\na specified size.", 
            "title": "Isilon"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_4", 
            "text": "The following is an example configuration of the Isilon driver.  isilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  group: groupname\n  password: password\n  volumePath: /libstorage\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true  For information on the equivalent environment variable and CLI flag names\nplease see the section on how configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#extra-parameters", 
            "text": "The following items are configurable specific to this driver.   volumePath  represents the location under  /ifs/volumes  to allow volumes to\n   be created and removed.  nfsHost  is the configurable NFS server hostname or IP (often a\n   SmartConnect name) used when mounting exports  dataSubnet  is the subnet the REX-Ray driver is running on. This is used\n   for the NFS export host ACLs.", 
            "title": "Extra Parameters"
        }, 
        {
            "location": "/user-guide/storage-providers/#optional-parameters", 
            "text": "The following items are not required, but available to this driver.   insecure  defaults to  false .  group  defaults to the group of the user specified in the configuration.\n   Only use this option if you need volumes to be created with a different\n   group.  volumePath  defaults to \"\". This will have all new volumes created directly\n   under  /ifs/volumes .  quotas  defaults to  false . Set to  true  if you have a SmartQuotas\n   license enabled.", 
            "title": "Optional Parameters"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_4", 
            "text": "To activate the Isilon driver please follow the instructions for activating storage drivers ,\nusing  isilon  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_4", 
            "text": "Below is a full  config.yml  file that works with Isilon.  libstorage:\n  server:\n    services:\n      isilon:\n        driver: isilon\n        isilon:\n          endpoint: https://endpoint:8080\n          insecure: true\n          username: username\n          password: password\n          volumePath: /libstorage\n          nfsHost: nfsHost\n          dataSubnet: subnet\n          quotas: true", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#instructions", 
            "text": "It is expected that the  volumePath  exists already within the Isilon system.\nThis example would reflect a directory create under  /ifs/volumes/libstorage \nfor created volumes. It is not necessary to export this volume. The  dataSubnet \nparameter is required so the Isilon driver can restrict access to attached\nvolumes to the host that REX-Ray is running on.  If  quotas  are enabled, a SmartQuotas license must also be enabled on the\nIsilon cluster for the capacity size functionality of  libStorage  to work.  A SnapshotIQ license must be enabled on the Isilon cluster for the snapshot\nfunctionality of  libStorage  to work.", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_1", 
            "text": "The Isilon driver is not without its caveats:   The account used to access the Isilon cluster must be in a role with the\n  following privileges:  Namespace Access (ISI_PRIV_NS_IFS_ACCESS)  Platform API (ISI_PRIV_LOGIN_PAPI)  NFS (ISI_PRIV_NFS)  Restore (ISI_PRIV_IFS_RESTORE)  Quota (ISI_PRIV_QUOTA)          (if  quotas  are enabled)  Snapshot (ISI_PRIV_SNAPSHOT)    (if snapshots are used)", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#scaleio", 
            "text": "The ScaleIO driver registers a storage driver named  scaleio  with the\nlibStorage service registry and is used to connect and manage ScaleIO storage.", 
            "title": "ScaleIO"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_4", 
            "text": "The ScaleIO  REST Gateway  is required for the driver to function.  The  libStorage  client or application that embeds the  libStorage  client\n   must reside on a host that has the SDC client installed. The command\n    /opt/emc/scaleio/sdc/bin/drv_cfg --query_guid  should be executable and\n   should return the local SDC GUID.  The  official \n   Oracle Java Runtime Environment (JRE) is required. During testing, use of the\n   Open Java Development Kit (JDK) resulted in unexpected errors.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_5", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.  scaleio:\n  endpoint:             https://host_ip/api\n  apiVersion:            2.0 \n  insecure:             false\n  useCerts:             true\n  userName:             admin\n  password:             mypassword\n  systemID:             0\n  systemName:           sysv\n  protectionDomainID:   0\n  protectionDomainName: corp\n  storagePoolID:        0\n  storagePoolName:      gold\n  thinOrThick:          ThinProvisioned", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_3", 
            "text": "The  apiVersion  can optionally be set here to force certain API behavior.\nThe default is to retrieve the endpoint API, and pass this version during calls.  insecure  should be set to  true  if you have not loaded the SSL\ncertificates on the host.  A successful wget or curl should be possible without\nSSL errors to the API  endpoint  in this case.  useCerts  should only be set if you want to leverage the internal SSL\ncertificates.  This would be useful if you are deploying the REX-Ray binary\non a host that does not have any certificates installed.  systemID  takes priority over  systemName .  protectionDomainID  takes priority over  protectionDomainName .  storagePoolID  takes priority over  storagePoolName .  thinkOrThick  determines whether to provision as the default ThinProvisioned , or  ThickProvisioned .   For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior_3", 
            "text": "The  storageType  field that is configured per volume is considered the\nScaleIO Storage Pool.  This can be configured by default with the  storagePool \nsetting.  It is important that you create unique names for your Storage Pools\non the same ScaleIO platform.  Otherwise, when specifying  storageType  it\nmay choose at random which  protectionDomain  the pool comes from.  The  availabilityZone  field represents the ScaleIO Protection Domain.", 
            "title": "Runtime Behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuring-the-gateway", 
            "text": "Install the  EMC-ScaleIO-gateway  package.  Edit the /opt/emc/scaleio/gateway/webapps/ROOT/WEB-INF/classes/gatewayUser.properties \nfile and append the proper MDM IP addresses to the following  mdm.ip.addresses= \nparameter.  By default the password is the same as your administrative MDM password.  Start the gateway  service scaleio-gateway start .  With 1.32 we have noticed a restart of the gateway may be necessary as well\nafter an initial install with  service scaleio-gateway restart .", 
            "title": "Configuring the Gateway"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_5", 
            "text": "To activate the ScaleIO driver please follow the instructions for activating storage drivers ,\nusing  scaleio  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting_3", 
            "text": "Verify your parameters for  system ,  protectionDomain , and storagePool  are correct.  Verify that have the ScaleIO SDC service installed with rpm -qa EMC-ScaleIO-sdc  Verify that the following command returns the local SDC GUID /opt/emc/scaleio/sdc/bin/drv_cfg --query_guid .  Ensure that you are able to open a TCP connection to the gateway with the\naddress that you will be supplying below in the  gateway_ip  parameter.  For\nexample  telnet gateway_ip 443  should open a successful connection.  Removing\nthe  EMC-ScaleIO-gateway  package and reinstalling can force re-creation of\nself-signed certs which may help resolve gateway problems.  Also try restarting\nthe gateway with  service scaleio-gateway restart .  Ensure that you have the correct authentication credentials for the gateway.\nThis can be done with a curl login. You should receive an authentication\ntoken in return. curl --insecure --user admin:XScaleio123 https://gw_ip:443/api/login  Please review the gateway log at /opt/emc/scaleio/gateway/logs/catalina.out  for errors.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_5", 
            "text": "Below is a full  config.yml  file that works with ScaleIO.  libstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n        scaleio:\n          endpoint: https://gateway_ip/api\n          insecure: true\n          userName: username\n          password: password\n          systemName: tenantName\n          protectionDomainName: protectionDomainName\n          storagePoolName: storagePoolName", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#digitalocean", 
            "text": "Thanks to the efforts of our tremendous community, libStorage also has built-in\nsupport for DigitalOcean!", 
            "title": "DigitalOcean"
        }, 
        {
            "location": "/user-guide/storage-providers/#do-block-storage", 
            "text": "The DigitalOcean Block Storage (DOBS) driver registers a driver named  dobs \nwith the libStorage service registry and is used to attach and mount\nDigitalOcean block storage devices to DigitalOcean instances.   Note  The DigitalOcean Block Storage driver currently only supports operating in local only  mode where the libStorage server must be running on the same\nhost as the client.", 
            "title": "DO Block Storage"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_5", 
            "text": "The DigitalOcean block storage driver has the following requirements:   Valid DigitalOcean account  Valid DigitalOcean  access token", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_6", 
            "text": "The following example illustrates a minimal configuration for the DigitalOcean\nblock storage driver:  dobs:\n  token:  123456\n  region: nyc1   Note  The standard environment variable for the DigitalOcean access token is DIGITALOCEAN_ACCESS_TOKEN . However, the environment variable mapped to\nthis driver's  dobs.token  property is  DOBS_TOKEN . This choice was made\nto ensure that the driver must be explicitly configured for access instead\nof detecting a default token that may not be intended for the driver.", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#fittedcloud", 
            "text": "Another example of the great community shared by the libStorage project, the\ntalented people at FittedCloud have provided a driver for their EBS optimizer.", 
            "title": "FittedCloud"
        }, 
        {
            "location": "/user-guide/storage-providers/#ebs-optimizer", 
            "text": "The FittedCloud EBS Optimizer driver registers a storage driver named fittedcloud  with the libStorage service registry and provides the ability to\nconnect and manage thin-provisioned EBS volumes for EC2 instances.   Note  This version of the FittedCloud driver only supports configurations where\nclient and server are on the same host.  The libStorage server must be\nrunning on each node along side with the FittedCloud Agent.    Note  This version of the FittedCloud driver does not support co-existing with the\nebs driver on the same host. As a result it also doesn't support optimizing\nexisting EBS volumes.   See the  Examples  section\nbelow for a running example.    Note  The FittedCloud driver does not yet support snapshots or tags.", 
            "title": "EBS Optimizer"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_6", 
            "text": "This driver has the following requirements:   AWS account  VPC - EBS can be accessed within VPC  AWS Credentials  FittedCloud Agent software", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#getting-started", 
            "text": "Before starting, please make sure to register as a user by visiting the\nFittedCloud  customer website .\nOnce an account is activated it will be assigned a user ID, which can be found\non the Settings page after logging into the web site.  The following commands will download and install the latest FittedCloud Agent\nsoftware. The flags  -o S -m  enable new thin volumes to be created via the\ndocker command instead of optimizing existing EBS volumes.\nPlease replace the  User ID  with a FittedCloud user ID.  $ curl -skSL 'https://customer.fittedcloud.com/downloadsoftware?ver=latest' \\\n  -o fcagent.run\n$ sudo bash ./fcagent.run -- -o S -m -d  User ID   Please refer to FittedCloud website  for more details.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_7", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.  ebs:\n  accessKey:      XXXXXXXXXX\n  secretKey:      XXXXXXXXXX\n  kmsKeyID:       abcd1234-a123-456a-a12b-a123b4cd56ef", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_4", 
            "text": "FittedCloud driver shares the ebs driver's configuration\nparameters.  The  accessKey  and  secretKey  configuration parameters are optional and\nshould be used when explicit AWS credentials configuration needs to be provided.\nFittedCloud driver uses official golang AWS SDK library and supports all other\nways of providing access credentials, like environment variables or instance\nprofile IAM permissions.  If the  kmsKeyID  field is specified it will be used as the encryption key for\nall volumes that are created with a truthy encryption request field.", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_6", 
            "text": "The following example illustrates how to configured the FittedCloud driver:  libstorage:\n  service:    fittedcloud\nebs:\n  accessKey:  XXXXXXXXXX\n  secretKey:  XXXXXXXXXX  Additional information on configuring the FittedCloud driver may be found\nat  this  location.", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#google", 
            "text": "libStorage ships with support for Google Compute Engine (GCE) as well.", 
            "title": "Google"
        }, 
        {
            "location": "/user-guide/storage-providers/#gce-persistent-disk", 
            "text": "The Google Compute Engine Persistent Disk (GCEPD) driver registers a driver\nnamed  gcepd  with the libStorage service registry and is used to connect and\nmount Google Compute Engine (GCE) persistent disks with GCE machine instances.", 
            "title": "GCE Persistent Disk"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_7", 
            "text": "GCE account  The libStorage server must be running on a GCE instance created with a Service\n  Account with appropriate permissions, or a Service Account credentials file\n  in JSON format must be supplied. If not using the Compute Engine default\n  Service Account with the Cloud Platform/\"all cloud APIs\" scope, create a new\n  Service Account via the  IAM Portal .\n  This Service Account requires the  Compute Engine/Instance Admin ,\n   Compute Engine/Storage Admin , and  Project/Service Account Actor  roles.\n  Then create/download a new private key in JSON format. see\n   creating a service account \n  for details. The libStorage service must be restarted in order for permissions\n  changes on a service account to take effect.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_8", 
            "text": "The following is an example with all possible fields configured. For a running\nexample see the  Examples  section.  gcepd:\n  keyfile: /etc/gcekey.json\n  zone: us-west1-b\n  defaultDiskType: pd-ssd\n  tag: rexray", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_5", 
            "text": "The  keyfile  parameter is optional. It specifies a path on disk to a file\n  containing the JSON-encoded Service Account credentials. This file can be\n  downloaded from the GCE web portal. If  keyfile  is specified, the GCE\n  instance's service account is not considered, and is not necessary. If\n   keyfile  is  not  specified, the application will try to lookup\n   application default credentials .\n  This has the effect of looking for credentials in the priority described\n   here .  The  zone  parameter is optional, and configures the driver to  only  allow\n  access to the given zone. Creating and listing disks from other zones will be\n  denied. If a zone is not specified, the zone from the client Instance ID will\n  be used when creating new disks.  The  defaultDiskType  parameter is optional and specifies what type of disk\n  to create, either  pd-standard  or  pd-ssd . When not specified, the default\n  is  pd-ssd .  The  tag  parameter is optional, and causes the driver to create or return\n  disks that have a matching tag. The tag is implemented by using the GCE\n  label functionality available in the beta API. The value of the  tag \n  parameter is used as the value for a label with the key  libstoragetag .\n  Use of this parameter is encouraged, as the driver will only return volumes\n  that have been created by the driver, which is most useful to eliminate\n  listing the boot disks of every GCE disk in your project/zone. If you wsih to\n  \"expose\" previously created disks to the  GCEPD  driver, you can edit the\n  labels on the existing disk to have a key of  libstoragetag  and a value\n  matching that given in  tag .", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior_4", 
            "text": "The GCEPD driver enforces the GCE requirements for disk sizing and naming.\n  Disks must be created with a minimum size of 10GB. Disk names must adhere to\n  the regular expression of  [a-z]([-a-z0-9]*[a-z0-9])? , which means the first\n  character must be a lowercase letter, and all following characters must be a\n  dash, lowercase letter, or digit, except the last character, which cannot be a\n  dash.  If the  zone  parameter is not specified in the driver configuration, and a\n  request is received to list all volumes that does not specify a zone in the\n  InstanceID header, volumes from all zones will be returned.  By default, all disks will be created with type  pd-ssd , which creates an SSD\n  based disk. If you wish to create disks that are not SSD-based, change the\n  default via the driver config, or the type can be changed at creation time by\n  using the  Type  field of the create request.", 
            "title": "Runtime behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_6", 
            "text": "To activate the GCEPD driver please follow the instructions for activating storage drivers , using  gcepd  as the\ndriver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting_4", 
            "text": "Make sure that the JSON credentials file as specified in the  keyfile \n  configuration parameter is present and accessible, or that you are running in\n  a GCE instance created with a Service Account attached. Whether using a\n   keyfile  or the Service Account associated with the GCE instance, the Service\n  Account must have the appropriate permissions as described in\n   Configuration Notes", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_7", 
            "text": "Below is a full  config.yml  that works with GCE  libstorage:\n  server:\n    services:\n      gcepd:\n        driver: gcepd\n        gcepd:\n          keyfile: /etc/gcekey.json\n          tag: rexray", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_2", 
            "text": "Snapshot and copy functionality is not yet implemented  Most GCE instances can have up to 64 TB of total persistent disk space\n  attached. Shared-core machine types or custom machine types with less than\n  3.75 GB of memory are limited to 3 TB of total persistent disk space. Total\n  persistent disk space for an instance includes the size of the root persistent\n  disk. You can attach up to 16 independent persistent disks to most instances,\n  but instances with shared-core machine types or custom machine types with less\n  than 3.75 GB of memory are limited to a maximum of 4 persistent disks,\n  including the root persistent disk. See\n   GCE Disks  docs for more\n  details.  If running libStorage server in a mode where volume mounts will not be\n  performed on the same host where libStorage server is running, it should be\n  possible to use a Service Account without the  Service Account Actor  role, but\n  this has not been tested. Note that if persistent disk mounts are to be\n  performed on  any  GCE instances that have a Service Account associated with\n  the, the  Service Account Actor  role is required.", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#microsoft", 
            "text": "Microsoft Azure support is included with libStorage as well.", 
            "title": "Microsoft"
        }, 
        {
            "location": "/user-guide/storage-providers/#azure-unmanaged-disk", 
            "text": "The Microsoft Azure Unmanaged Disk (Azure UD) driver registers a driver\nnamed  azureud  with the libStorage service registry and is used to connect and\nmount Azure unmanaged disks from Azure page blob storage with Azure virtual\nmachines.", 
            "title": "Azure Unmanaged Disk"
        }, 
        {
            "location": "/user-guide/storage-providers/#requirements_8", 
            "text": "An Azure account  An Azure subscription  An Azure storage account  An Azure resource group  Any virtual machine where disks are going to be attached must have the\n   lsscsi  utility installed. You can install this with  yum install lsscsi  on\n  Red Hat based distributions, or with  apt-get install lsscsi  on Debian based\n  distributions.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_9", 
            "text": "The following is an example with all possible fields configured. For a running\nexample see the  Examples  section.  azureud:\n  subscriptionID: abcdef01-2345-6789-abcd-ef0123456789\n  resourceGroup: testgroup\n  tenantID: usernamehotmail.onmicrosoft.com\n  storageAccount: username\n  storageAccessKey: XXXXXXXX\n  clientID: 123def01-2345-6789-abcd-ef0123456789\n  clientSecret: XXXXXXXX\n  certPath:\n  container: vhds\n  useHTTPS: true", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes_6", 
            "text": "subscriptionID  is required, and is the UUID of your Azure subscription  resourceGroup  is required, and is the name of the resource group for your\n  VMs and storage.  tenantID  is required, and is either the domain or UUID for your active\n  directory account within Azure.  storageAccount  is required, and is the name of the storage account where\n  your disks will be created.  storageAccessKey  is required, and is a valid access key associated with the\n   storageAccount .  clientID  is required, and is the UUID of your client, which was created as\n  an App Registration within your Azure active directory account.  clientSecret  is required if  certPath  is not provided instead. It is a\n  valid access key associated with  clientID , and is managed as part of the App\n  Registration.  certPath  is an alternative to  clientSecret , contains the location of a\n  PKCS encoded RSA private key associated with  clientID .  container  is optional, and specifies the name of an existing container\n  within  storageAccount . This container must already exist and is not created\n  automatically.  useHTTPS  is optional, and is a boolean value on whether to use HTTPS when\n  communicating with the Azure storage endpoint.", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior_5", 
            "text": "The  container  config option defaults to  vhds , and this container is\n  present by default in Azure. Changing this option is only necessary if you\n  want to use a different container within your storage account.  Volume Attach/Detach operations in Azure take a long time, sometimes greater\n  than 60 seconds, which is libStorage's default task timeout. When the timeout\n  is hit, libStorage returns information to the caller about a queued task, and\n  that the task is still running. This may cause issues for upstream callers.\n  It is  highly  recommended to adjust this default timeout to 120 seconds by\n  setting the  libstorage.server.tasks.exeTimeout  property. This is done in\n  the  Examples  section below.", 
            "title": "Runtime Behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_7", 
            "text": "To activate the Azure UD driver please follow the instructions for activating storage drivers , using  azureud  as\nthe driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting_5", 
            "text": "After creating your app registration, you must go into the\n   Required Permissions  tab and grant access to \"Windows Azure Service\n  Management API\". Choose the delegated permission for accessing as organization\n  users.  You must also grant your app registration access to your subscription, by\n  going to Subscriptions- Your  subscriptionID - Access Control (IAM). From\n  there, add your app registration as a user, which you will have to search for\n  by name. Grant the role of \"Owner\".", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_8", 
            "text": "Below is a full  config.yml  that works with Azure UD  libstorage:\n  server:\n    tasks:\n      exeTimeout: 120s\n    services:\n      azure:\n        driver: azureud\n        azureud:\n          subscriptionID: abcdef01-2345-6789-abcd-ef0123456789\n          resourceGroup: testgroup\n          tenantID: usernamehotmail.onmicrosoft.com\n          storageAccount: username\n          storageAccessKey: XXXXXXXX\n          clientID: 123def01-2345-6789-abcd-ef0123456789\n          clientSecret: XXXXXXXX", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_3", 
            "text": "Snapshot and Copy functionality is not yet implemented  The number of disks you can attach to a Virtual Machine depends on its type.  Good resources for reading about disks in Azure are\n   here \n  and  here .", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#virtualbox", 
            "text": "The VirtualBox driver registers a storage driver named  virtualbox  with the\nlibStorage service registry and is used by VirtualBox's VMs to connect and\nmanage volumes provided by VirtualBox.", 
            "title": "VirtualBox"
        }, 
        {
            "location": "/user-guide/storage-providers/#prerequisites", 
            "text": "In order to leverage the  virtualbox  driver, the  libStorage  client or must\nbe located on each VM that you wish to be able to consume external volumes.\nThe driver leverages the  vboxwebserv  HTTP SOAP API which is a process that\nmust be started from the VirtualBox  host  (ie OS X) using vboxwebsrv -H 0.0.0.0 -v  or additionally with  -b  for running in the\nbackground. This allows the VMs running  libStorage  to remotely make calls to\nthe underlying VirtualBox application. A test for connectivity can be done with telnet virtualboxip 18083  from the VM. The  virtualboxip  is what you\nwould put in the  endpoint  value.  Leveraging authentication for the VirtualBox webserver is optiona.. The HTTP\nSOAP API can have authentication disabled by running VBoxManage setproperty websrvauthlibrary null .  Hot-Plugging is required, which limits the usefulness of this driver to  SATA \nonly.  Ensure that your VM has  pre-created  this controller and it is\nnamed  SATA .  Otherwise the  controllerName  field must be populated\nwith the name of the controller you wish to use.  The port count must be set\nmanually as it cannot be increased when the VMs are on.  A count of  30 \nis suggested.  VirtualBox 5.0.10+ must be used.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_10", 
            "text": "The following is an example configuration of the VirtualBox driver. \nThe  localMachineNameOrId  parameter is for development use where you force libStorage  to use a specific VM identity.  Choose a  volumePath  to store the\nvolume files or virtual disks.  This path should be created ahead of time.  virtualbox:\n  endpoint: http://virtualboxhost:18083\n  userName: optional\n  password: optional\n  tls: false\n  volumePath: $HOME/VirtualBox/Volumes\n  controllerName: name\n  localMachineNameOrId: forDevelopmentUse  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_8", 
            "text": "To activate the VirtualBox driver please follow the instructions for activating storage drivers ,\nusing  virtualbox  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_9", 
            "text": "Below is a working  config.yml  file that works with VirtualBox.  libstorage:\n  server:\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_4", 
            "text": "Snapshot and create volume from volume functionality is not available yet\n  with this driver.  The driver supports VirtualBox 5.0.10+", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/schedulers/", 
            "text": "Schedulers\n\n\nScheduling storage one resource at a time...\n\n\n\n\nOverview\n\n\nThis page reviews the scheduling systems supported by \nlibStorage\n.\n\n\nDocker\n\n\nlibStorage\n's '\nDocker Integration Driver\n is compatible with 1.10+.\n\n\nHowever, \nDocker 1.10.2+\n is suggested if volumes are shared between containers\nor interactive volume inspection requests are desired via the \n/volumes\n,\n\n/volumes/{service}\n, and  \n/volumes/{service}/{volumeID}\n resources.\n\n\nPlease  note that this is \nnot\n the same as\n\nDocker's Volume Plug-in\n.\n\nlibStorage\n does not provide a way to expose the \nDocker Integration Driver\n\nvia the \nDocker Volume Plug-in\n, but \nREX-Ray\n, which embeds \nlibStorage\n,\ndoes.\n\n\nExample Configuration\n\n\nBelow is an example \nconfig.yml\n that can be used.  The \nvolume.mount.preempt\n\nis an optional parameter here which enables any host to take control of a\nvolume irrespective of whether other hosts are using the volume.  If this is\nset to \nfalse\n then plugins should ensure \nsafety\n first by locking the\nvolume from to the current owner host. We also specify \ndocker.size\n which will\ncreate all new volumes at the specified size in GB.\n\n\nlibstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  integration:\n    volume:\n      mount:\n        preempt: true\n      create:\n        default:\n          size: 1 # GB\n  server:\n    endpoints:\n      localhost:\n        address: unix:///var/run/libstorage/localhost.sock\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA\n\n\n\n\nConfiguration Properties\n\n\nThe Docker integration driver adheres to the properties described in the\nsection on an\n\nIntegration driver's volume-related properties\n.\n\n\nPlease note that with \nDocker\n 1.9.1 or below, it is recommended that the\nproperty \nlibstorage.integration.volume.remove.disable\n be set to \ntrue\n in\norder to prevent \nDocker\n from removing external volumes in-use by containers\nthat are forcefully removed.\n\n\nCaveats\n\n\nIf you restart the process which embeds \nlibStorage\n and hosts the\n\nDocker Volume Plug-in\n while volumes \nare shared between Docker containers\n,\nthen problems may arise when stopping one of the containers sharing the volume.\n\n\nIt is suggested to avoid stopping these containers at this point until all\ncontainers sharing the volumes can be stopped. This will enable the unmount\nprocess to proceed cleanly.", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#schedulers", 
            "text": "Scheduling storage one resource at a time...", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#overview", 
            "text": "This page reviews the scheduling systems supported by  libStorage .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/schedulers/#docker", 
            "text": "libStorage 's ' Docker Integration Driver  is compatible with 1.10+.  However,  Docker 1.10.2+  is suggested if volumes are shared between containers\nor interactive volume inspection requests are desired via the  /volumes , /volumes/{service} , and   /volumes/{service}/{volumeID}  resources.  Please  note that this is  not  the same as Docker's Volume Plug-in . libStorage  does not provide a way to expose the  Docker Integration Driver \nvia the  Docker Volume Plug-in , but  REX-Ray , which embeds  libStorage ,\ndoes.", 
            "title": "Docker"
        }, 
        {
            "location": "/user-guide/schedulers/#example-configuration", 
            "text": "Below is an example  config.yml  that can be used.  The  volume.mount.preempt \nis an optional parameter here which enables any host to take control of a\nvolume irrespective of whether other hosts are using the volume.  If this is\nset to  false  then plugins should ensure  safety  first by locking the\nvolume from to the current owner host. We also specify  docker.size  which will\ncreate all new volumes at the specified size in GB.  libstorage:\n  host: unix:///var/run/libstorage/localhost.sock\n  integration:\n    volume:\n      mount:\n        preempt: true\n      create:\n        default:\n          size: 1 # GB\n  server:\n    endpoints:\n      localhost:\n        address: unix:///var/run/libstorage/localhost.sock\n    services:\n      virtualbox:\n        driver: virtualbox\n        virtualbox:\n          endpoint:       http://10.0.2.2:18083\n          tls:            false\n          volumePath:     $HOME/VirtualBox/Volumes\n          controllerName: SATA", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/user-guide/schedulers/#configuration-properties", 
            "text": "The Docker integration driver adheres to the properties described in the\nsection on an Integration driver's volume-related properties .  Please note that with  Docker  1.9.1 or below, it is recommended that the\nproperty  libstorage.integration.volume.remove.disable  be set to  true  in\norder to prevent  Docker  from removing external volumes in-use by containers\nthat are forcefully removed.", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/schedulers/#caveats", 
            "text": "If you restart the process which embeds  libStorage  and hosts the Docker Volume Plug-in  while volumes  are shared between Docker containers ,\nthen problems may arise when stopping one of the containers sharing the volume.  It is suggested to avoid stopping these containers at this point until all\ncontainers sharing the volumes can be stopped. This will enable the unmount\nprocess to proceed cleanly.", 
            "title": "Caveats"
        }, 
        {
            "location": "/dev-guide/project-guidelines/", 
            "text": "Project Guidelines\n\n\nThese are important.\n\n\n\n\nPeople contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.\n\n\nDocumentation\n\n\nThere are two types of documentation: source and markdown.\n\n\nSource Code\n\n\nAll source code should be documented in accordance with the\n\nGo's documentation rules\n.\n\n\nMarkdown\n\n\nWhen creating or modifying the project's \nREADME.md\n file or any of the\ndocumentation in the \n.docs\n directory, please keep the following rules in\nmind:\n\n\n\n\nAll links to internal resources should be relative.\n\n\nAll links to markdown files should include the file extension.\n\n\n\n\nFor example, the below link points to the anchor \nbasic-configuration\n on the\n\nConfiguration\n page:\n\n\n\n\n/user-guide/config#basic-configuration\n\n\nHowever, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.\n\n\nWhile it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:\n\n\n\n\n../user-guide/config.md#basic-configuration\n\n\nNow the link will work regardless from where it's viewed.\n\n\nStyle \n Syntax\n\n\nAll source files should be processed by the\n\ngometalinter\n before committed. Any\nerrors or warnings produced by the tools should be corrected before the source\nis committed.\n\n\nIf \nAtom\n is your IDE of choice, install the\n\ngo-plus\n package, and it will execute the\ngometalinter every time a source file is saved.\n\n\nIn lieu of using Atom as the IDE, the project's \nMakefile\n automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.\n\n\nCode Coverage\n\n\nAll new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.\n\n\nThis project uses\n\nCodecov\n for code coverage, and\nall pull requests are processed just as a build from \nmaster\n. If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.\n\n\nCommit Messages\n\n\nCommit messages should follow the guide \n5 Useful Tips For a Better Commit\nMessage\n.\nThe two primary rules to which to adhere are:\n\n\n\n\n\n\nCommit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.\n\n\n\n\n\n\nThe commit message's body should not have a width that exceeds 72\n     characters.\n\n\n\n\n\n\nFor example, the following commit has a very useful message that is succinct\nwithout losing utility.\n\n\ncommit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz \nsakutz@gmail.com\n\nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.\n\n\n\n\nPlease note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s\n -s\nAdded --format,-f option for CLI\n\n\n\n\nIt's also equally simple to print the commit's subject and body together:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s%n%n%b\n -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.\n\n\n\n\nSubmitting Changes\n\n\nAll developers are required to follow the\n\nGitHub Flow model\n when\nproposing new features or even submitting fixes.\n\n\nPlease note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a \nfork\n of this project, not from within a\nbranch of this project itself.\n\n\nPull requests submitted to this project should adhere to the following\nguidelines:\n\n\n\n\n\n\nBranches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.\n\n\n\n\n\n\nUnless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#project-guidelines", 
            "text": "These are important.   People contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#documentation", 
            "text": "There are two types of documentation: source and markdown.", 
            "title": "Documentation"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#source-code", 
            "text": "All source code should be documented in accordance with the Go's documentation rules .", 
            "title": "Source Code"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#markdown", 
            "text": "When creating or modifying the project's  README.md  file or any of the\ndocumentation in the  .docs  directory, please keep the following rules in\nmind:   All links to internal resources should be relative.  All links to markdown files should include the file extension.   For example, the below link points to the anchor  basic-configuration  on the Configuration  page:   /user-guide/config#basic-configuration  However, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.  While it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:   ../user-guide/config.md#basic-configuration  Now the link will work regardless from where it's viewed.", 
            "title": "Markdown"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#style-syntax", 
            "text": "All source files should be processed by the gometalinter  before committed. Any\nerrors or warnings produced by the tools should be corrected before the source\nis committed.  If  Atom  is your IDE of choice, install the go-plus  package, and it will execute the\ngometalinter every time a source file is saved.  In lieu of using Atom as the IDE, the project's  Makefile  automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.", 
            "title": "Style &amp; Syntax"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#code-coverage", 
            "text": "All new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.  This project uses Codecov  for code coverage, and\nall pull requests are processed just as a build from  master . If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.", 
            "title": "Code Coverage"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#commit-messages", 
            "text": "Commit messages should follow the guide  5 Useful Tips For a Better Commit\nMessage .\nThe two primary rules to which to adhere are:    Commit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.    The commit message's body should not have a width that exceeds 72\n     characters.    For example, the following commit has a very useful message that is succinct\nwithout losing utility.  commit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz  sakutz@gmail.com \nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.  Please note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s  -s\nAdded --format,-f option for CLI  It's also equally simple to print the commit's subject and body together:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s%n%n%b  -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.", 
            "title": "Commit Messages"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#submitting-changes", 
            "text": "All developers are required to follow the GitHub Flow model  when\nproposing new features or even submitting fixes.  Please note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a  fork  of this project, not from within a\nbranch of this project itself.  Pull requests submitted to this project should adhere to the following\nguidelines:    Branches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.    Unless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Submitting Changes"
        }, 
        {
            "location": "/dev-guide/build-reference/", 
            "text": "Build Reference\n\n\nHow to build libStorage\n\n\n\n\nBasic Builds\n\n\nThe following one-line command is the quickest, simplest, and most\ndeterministic approach to building libStorage:\n\n\n$ git clone https://github.com/codedellemc/libstorage \n make -C libstorage\n\n\n\n\n\n\nnote\n\n\nThe above \nmake\n command defaults to the \ndocker-build\n target only if\nDocker is detected and running on a host, otherwise the \nbuild\n target is\nused. For more information about the \nbuild\n target, please see the\n\nAdvanced Builds\n section.\n\n\n\n\nBasic Build Requirements\n\n\nBuilding libStorage with Docker has the following requirements:\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nDocker\n\n\n=1.11\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\nGit\n\n\n= 1.7\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nBasic Build Targets\n\n\nThe following targets are available when building libStorage with Docker:\n\n\n\n\n\n\n\n\nTarget\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndocker-build\n\n\nBuilds libStorage inside a Docker container.\n\n\n\n\n\n\ndocker-test\n\n\nExecutes all of the libStorage tests inside the container.\n\n\n\n\n\n\ndocker-clean\n\n\nThis target stops and removes the default container used for libStorage builds. The name of the default container is \nbuild-libstorage\n.\n\n\n\n\n\n\ndocker-clobber\n\n\nThis target stops and removes all Docker containers that have a name that matches the name of the configured container prefix (default prefix is \nbuild-libstorage\n).\n\n\n\n\n\n\ndocker-list\n\n\nLists all Docker containers that have a name that matches the name of the configured prefix (default prefix is \nbuild-libstorage\n).\n\n\n\n\n\n\n\n\nBasic Build Options\n\n\nThe following options (via environment variables) can be used to influence\nhow libStorage is built with Docker:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDRIVERS\n\n\nThis variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command \n$ DRIVERS=\"ebs scaleio\" make docker-build\n would build libStorage for only the EBS and ScaleIO storage platforms.\n\n\n\n\n\n\nDBUILD_ONCE\n\n\nWhen set to \n1\n, this environment variable instructs the Makefile to create a temporary, one-time use container for the subsequent build. The container is removed upon a successful build. If the build fails the container is not removed. This is because Makefile error logic is lacking. However, \nmake docker-clobber\n can be used to easily clean up these containers. The containers will follow a given pattern using the container prefix (\nbuild-libstorage\n is the default prefix value). The one-time containers use \nPREFIX-EPOCH\n. For example, \nbuild-libstorage-1474691232\n.\n\n\n\n\n\n\nDGOOS\n\n\nThis sets the OS target for which to build the libStorage binaries. Valid values are \nlinux\n and \ndarwin\n. If omitted the host OS value returned from \nuname -s\n is used instead.\n\n\n\n\n\n\nDLOCAL_IMPORTS\n\n\nSpecify a list of space-delimited import paths that will be copied from the host OS's \nGOPATH\n into the container build's vendor area, overriding the dependency code that would normally be fetched by Glide.\nFor example, the project's \nglide.yaml\n file might specify to build REX-Ray with libStorage v0.2.1. However, the following command will build REX-Ray using the libStorage sources on the host OS at \n$GOPATH/src/github.com/codedellemc/libstorage\n:\n$ DLOCAL_IMPORTS=github.com/codedellemc/libstorage make docker-build\nUsing local sources can sometimes present a problem due to missing dependencies. Please see the next environment variable for instructions on how to overcome this issue.\n\n\n\n\n\n\nDGLIDE_YAML\n\n\nSpecify a file that will be used for the container build in place of the standard \nglide.yaml\n file.\nThis is necessary for occasions when sources injected into the build via the \nDLOCAL_IMPORTS\n variable import packages that are not imported by the package specified in the project's standard \nglide.yaml\n file.\nFor example, if \nglide.yaml\n specifies that libStorage depends upon AWS SDK v1.2.2, but \nDLOCAL_IMPORTS\n specifies the value \ngithub.com/aws/aws-sdk-go\n and the AWS SDK source code on the host includes a new dependency not present in the v1.2.2 version, Glide will not fetch the new dependency when doing the container build.\nSo it may be necessary to use \nDGLIDE_YAML\n to provide a superset of the project's standard \nglide.yaml\n file which also includes the dependencies necessary to build the packages specified in \nDLOCAL_IMPORTS\n.\n\n\n\n\n\n\n\n\nAdvanced Builds\n\n\nWhile building libStorage with Docker is simple, it ultimately relies on the\nsame \nMakefile\n included in the libStorage repository and so it's entirely\npossible (and often desirable) to build libStorage directly.\n\n\nAdvanced Build Requirements\n\n\nThis project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to\n\nbuild\n \nlibStorage\n, not run it.\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nGo\n\n\n=1.6\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\nGlide\n\n\n=0.10\n\n\n\n\n\n\nX-Code Command Line Tools (OS X only)\n\n\n= OS X 10.9\n\n\n\n\n\n\nLinux Kernel Headers (Linux only)\n\n\n=Linux Kernel 3.13\n\n\n\n\n\n\nGNU C Compiler\n (Linux only)\n\n\n= 4.8\n\n\n\n\n\n\nPerl\n\n\n= 5.0\n\n\n\n\n\n\nGit\n\n\n= 1.7\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nIt's also possible to use GCC as the Cgo compiler for OS X or to use Clang on\nLinux, but by default Clang is used on OS X and GCC on Linux.\n\n\nAdvanced Build Targets\n\n\nThe following targets are available when building libStorage directly:\n\n\n\n\n\n\n\n\nTarget\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbuild\n\n\nBuilds libStorage.\n\n\n\n\n\n\ntest\n\n\nExecutes all of the libStorage tests.\n\n\n\n\n\n\nclean\n\n\nThis target removes all of the source file markers.\n\n\n\n\n\n\nclobber\n\n\nThis is the same as \nclean\n but also removes any produced artifacts.\n\n\n\n\n\n\n\n\nAdvanced Build Options\n\n\nThe following options (via environment variables) can be used to influence\nhow libStorage is built:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDRIVERS\n\n\nThis variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command \n$ DRIVERS=\"ebs scaleio\" make build\n would build libStorage for only the EBS and ScaleIO storage platforms.\n\n\n\n\n\n\n\n\nVersion File\n\n\nThere is a file at the root of the project named \nVERSION\n. The file contains\na single line with the \ntarget\n version of the project in the file. The version\nfollows the format:\n\n\n(?\nmajor\n\\d+)\\.(?\nminor\n\\d+)\\.(?\npatch\n\\d+)(-rc\\d+)?\n\n\nFor example, during active development of version \n0.1.0\n the file would\ncontain the version \n0.1.0\n. When it's time to create \n0.1.0\n's first\nrelease candidate the version in the file will be changed to \n0.1.0-rc1\n. And\nwhen it's time to release \n0.1.0\n the version is changed back to \n0.1.0\n.\n\n\nSo what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the \nVERSION\n file in fact has two purposes:\n\n\n\n\n\n\nFirst and foremost updating the \nVERSION\n file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of \nmaster\n would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the \nVERSION\n file is much cleaner.\n\n\n\n\n\n\nThe contents of the \nVERSION\n file are also used during the build process\n     as a means of overriding the output of a \ngit describe\n. This enables the\n     semantic version injected into the produced binary to be created using\n     the \ntargeted\n version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-reference", 
            "text": "How to build libStorage", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-builds", 
            "text": "The following one-line command is the quickest, simplest, and most\ndeterministic approach to building libStorage:  $ git clone https://github.com/codedellemc/libstorage   make -C libstorage   note  The above  make  command defaults to the  docker-build  target only if\nDocker is detected and running on a host, otherwise the  build  target is\nused. For more information about the  build  target, please see the Advanced Builds  section.", 
            "title": "Basic Builds"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-requirements", 
            "text": "Building libStorage with Docker has the following requirements:     Requirement  Version      Operating System  Linux, OS X    Docker  =1.11    GNU Make  =3.80    Git  = 1.7     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.", 
            "title": "Basic Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-targets", 
            "text": "The following targets are available when building libStorage with Docker:     Target  Description      docker-build  Builds libStorage inside a Docker container.    docker-test  Executes all of the libStorage tests inside the container.    docker-clean  This target stops and removes the default container used for libStorage builds. The name of the default container is  build-libstorage .    docker-clobber  This target stops and removes all Docker containers that have a name that matches the name of the configured container prefix (default prefix is  build-libstorage ).    docker-list  Lists all Docker containers that have a name that matches the name of the configured prefix (default prefix is  build-libstorage ).", 
            "title": "Basic Build Targets"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-options", 
            "text": "The following options (via environment variables) can be used to influence\nhow libStorage is built with Docker:     Environment Variable  Description      DRIVERS  This variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command  $ DRIVERS=\"ebs scaleio\" make docker-build  would build libStorage for only the EBS and ScaleIO storage platforms.    DBUILD_ONCE  When set to  1 , this environment variable instructs the Makefile to create a temporary, one-time use container for the subsequent build. The container is removed upon a successful build. If the build fails the container is not removed. This is because Makefile error logic is lacking. However,  make docker-clobber  can be used to easily clean up these containers. The containers will follow a given pattern using the container prefix ( build-libstorage  is the default prefix value). The one-time containers use  PREFIX-EPOCH . For example,  build-libstorage-1474691232 .    DGOOS  This sets the OS target for which to build the libStorage binaries. Valid values are  linux  and  darwin . If omitted the host OS value returned from  uname -s  is used instead.    DLOCAL_IMPORTS  Specify a list of space-delimited import paths that will be copied from the host OS's  GOPATH  into the container build's vendor area, overriding the dependency code that would normally be fetched by Glide. For example, the project's  glide.yaml  file might specify to build REX-Ray with libStorage v0.2.1. However, the following command will build REX-Ray using the libStorage sources on the host OS at  $GOPATH/src/github.com/codedellemc/libstorage : $ DLOCAL_IMPORTS=github.com/codedellemc/libstorage make docker-build Using local sources can sometimes present a problem due to missing dependencies. Please see the next environment variable for instructions on how to overcome this issue.    DGLIDE_YAML  Specify a file that will be used for the container build in place of the standard  glide.yaml  file. This is necessary for occasions when sources injected into the build via the  DLOCAL_IMPORTS  variable import packages that are not imported by the package specified in the project's standard  glide.yaml  file. For example, if  glide.yaml  specifies that libStorage depends upon AWS SDK v1.2.2, but  DLOCAL_IMPORTS  specifies the value  github.com/aws/aws-sdk-go  and the AWS SDK source code on the host includes a new dependency not present in the v1.2.2 version, Glide will not fetch the new dependency when doing the container build. So it may be necessary to use  DGLIDE_YAML  to provide a superset of the project's standard  glide.yaml  file which also includes the dependencies necessary to build the packages specified in  DLOCAL_IMPORTS .", 
            "title": "Basic Build Options"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-builds", 
            "text": "While building libStorage with Docker is simple, it ultimately relies on the\nsame  Makefile  included in the libStorage repository and so it's entirely\npossible (and often desirable) to build libStorage directly.", 
            "title": "Advanced Builds"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-requirements", 
            "text": "This project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to build   libStorage , not run it.     Requirement  Version      Operating System  Linux, OS X    Go  =1.6    GNU Make  =3.80    Glide  =0.10    X-Code Command Line Tools (OS X only)  = OS X 10.9    Linux Kernel Headers (Linux only)  =Linux Kernel 3.13    GNU C Compiler  (Linux only)  = 4.8    Perl  = 5.0    Git  = 1.7     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.  It's also possible to use GCC as the Cgo compiler for OS X or to use Clang on\nLinux, but by default Clang is used on OS X and GCC on Linux.", 
            "title": "Advanced Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-targets", 
            "text": "The following targets are available when building libStorage directly:     Target  Description      build  Builds libStorage.    test  Executes all of the libStorage tests.    clean  This target removes all of the source file markers.    clobber  This is the same as  clean  but also removes any produced artifacts.", 
            "title": "Advanced Build Targets"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-options", 
            "text": "The following options (via environment variables) can be used to influence\nhow libStorage is built:     Environment Variable  Description      DRIVERS  This variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command  $ DRIVERS=\"ebs scaleio\" make build  would build libStorage for only the EBS and ScaleIO storage platforms.", 
            "title": "Advanced Build Options"
        }, 
        {
            "location": "/dev-guide/build-reference/#version-file", 
            "text": "There is a file at the root of the project named  VERSION . The file contains\na single line with the  target  version of the project in the file. The version\nfollows the format:  (? major \\d+)\\.(? minor \\d+)\\.(? patch \\d+)(-rc\\d+)?  For example, during active development of version  0.1.0  the file would\ncontain the version  0.1.0 . When it's time to create  0.1.0 's first\nrelease candidate the version in the file will be changed to  0.1.0-rc1 . And\nwhen it's time to release  0.1.0  the version is changed back to  0.1.0 .  So what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the  VERSION  file in fact has two purposes:    First and foremost updating the  VERSION  file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of  master  would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the  VERSION  file is much cleaner.    The contents of the  VERSION  file are also used during the build process\n     as a means of overriding the output of a  git describe . This enables the\n     semantic version injected into the produced binary to be created using\n     the  targeted  version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Version File"
        }, 
        {
            "location": "/dev-guide/release-process/", 
            "text": "Release Process\n\n\nHow to release libStorage\n\n\n\n\nProject Stages\n\n\nThis project has three parallels stages of release:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nunstable\n\n\nThe tip or HEAD of the \nmaster\n branch is referred to as \nunstable\n\n\n\n\n\n\nstaged\n\n\nA commit tagged with the suffix \n-rc\\d+\n such as \nv0.1.0-rc2\n is a \nstaged\n release. These are release candidates.\n\n\n\n\n\n\nstable\n\n\nA commit tagged with a version sans \n-rc\\d+\n suffix such as \nv0.1.0\n is a \nstable\n release.\n\n\n\n\n\n\n\n\nThere are no steps necessary to create an \nunstable\n release as that happens\nautomatically whenever an untagged commit is pushed to \nmaster\n. However, the\nfollowing workflow should be used when tagging a \nstaged\n release candidate\nor \nstable\n release.\n\n\n\n\nReview outstanding issues \n pull requests\n\n\nPrepare release notes\n\n\nUpdate the version file\n\n\nCommit \n pull request\n\n\nTag the release\n\n\n\n\nReview Issues \n Pull Requests\n\n\nThe first step to a release is to review the outstanding\n\nissues\n and\n\npull requests\n that are tagged\nfor the release in question.\n\n\nIf there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.\n\n\nIt is \nhighly\n recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of \nmaster\n. Remember, while\nGitHub will update a pull request as in conflict if a change to \nmaster\n\nresults in a merge conflict with the pull request, GitHub will \nnot\n force a\nnew build to spawn unless the pull request is actually updated.\n\n\nAt the very minimum a pull request's build should be re-executed prior to the\npull request being merged if \nmaster\n has changed since the pull request was\nopened.\n\n\nPrepare Release Notes\n\n\nUpdate the release notes at \n.docs/about/release-notes.md\n. This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.\n\n\nThe most recent, \nstable\n version of the release notes are always available\nonline at\n\nlibStorage's documentation site\n.\n\n\nUpdate Version File\n\n\nThe \nVERSION\n file exists at the root of the project and should be updated to\nreflect the value of the intended release.\n\n\nFor example, if creating the first release candidate for version 0.1.0, the\ncontents of the \nVERSION\n file should be a single line \n0.1.0-rc1\n followed by\na newline character:\n\n\n$ cat VERSION\n0.1.0-rc1\n\n\n\n\nIf releasing version 0.1.0 proper then the contents of the \nVERSION\n file\nshould be \n0.1.0\n followed by a newline character:\n\n\n$ cat VERSION\n0.1.0\n\n\n\n\nCommit \n Pull Request\n\n\nOnce all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.\n\n\nPlease make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.\n\n\nA release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:\n\n\nRelease Candidate 0.1.0-rc1\n\nThis patch marks release candidate 0.1.0-rc1.\n\n\n\n\nIf the commit message is longer it should simply reflect the same information\nfrom the release notes.\n\n\nOnce committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.\n\n\nTag the Release\n\n\nOnce the pull request marking the \nstaged\n or \nstable\n release has been merged\ninto \nupstream\n's \nmaster\n it's time to tag the release.\n\n\nTag Format\n\n\nThe release tag should follow a prescribed format depending upon the release\ntype:\n\n\n\n\n\n\n\n\nRelease Type\n\n\nTag Format\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nstaged\n\n\nvMAJOR.MINOR.PATCH-rc[0-9]\n\n\nv0.1.0-rc1\n\n\n\n\n\n\nstable\n\n\nvMAJOR.MINOR-PATCH\n\n\nv0.1.0\n\n\n\n\n\n\n\n\nTag Methods\n\n\nThere are two ways to tag a release:\n\n\n\n\nGitHub Releases\n\n\nCommand Line\n\n\n\n\nCommand Line\n\n\nIf tagging a release via the command line be sure to fetch the latest changes\nfrom \nupstream\n's \nmaster\n and either merge them into your local copy of\n\nmaster\n or reset the local copy to reflect \nupstream\n prior to creating\nany tags.\n\n\nThe following combination of commands can be used to create a tag for\n0.1.0 Release Candidate 1:\n\n\ngit fetch upstream \n \\\n  git checkout master \n \\\n  git reset --hard upstream/master \n \\\n  git tag -a -m v0.1.0-rc1 v0.1.0-rc1\n\n\n\n\nThe above example combines a few operations:\n\n\n\n\nThe first command fetches the \nupstream\n changes\n\n\nThe local \nmaster\n branch is checked out\n\n\nThe local \nmaster\n branch is hard reset to \nupstream/master\n\n\nAn annotated tag is created on \nmaster\n for \nv0.1.0-rc1\n, or 0.1.0 Release\n     Candidate 1, with a tag message of \nv0.1.0-rc1\n.\n\n\n\n\nPlease note that the third step will erase any changes that exist only in the\nlocal \nmaster\n branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.\n\n\nThe above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a \nstaged\n or\n\nstable\n release. For \nstable\n releases the project's documentation will also be\nupdated.\n\n\nOnce positive everything looks good simply execute the following command to\npush the tag to the \nupstream\n repository:\n\n\ngit push upstream v0.1.0-rc1", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#release-process", 
            "text": "How to release libStorage", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#project-stages", 
            "text": "This project has three parallels stages of release:     Name  Description      unstable  The tip or HEAD of the  master  branch is referred to as  unstable    staged  A commit tagged with the suffix  -rc\\d+  such as  v0.1.0-rc2  is a  staged  release. These are release candidates.    stable  A commit tagged with a version sans  -rc\\d+  suffix such as  v0.1.0  is a  stable  release.     There are no steps necessary to create an  unstable  release as that happens\nautomatically whenever an untagged commit is pushed to  master . However, the\nfollowing workflow should be used when tagging a  staged  release candidate\nor  stable  release.   Review outstanding issues   pull requests  Prepare release notes  Update the version file  Commit   pull request  Tag the release", 
            "title": "Project Stages"
        }, 
        {
            "location": "/dev-guide/release-process/#review-issues-pull-requests", 
            "text": "The first step to a release is to review the outstanding issues  and pull requests  that are tagged\nfor the release in question.  If there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.  It is  highly  recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of  master . Remember, while\nGitHub will update a pull request as in conflict if a change to  master \nresults in a merge conflict with the pull request, GitHub will  not  force a\nnew build to spawn unless the pull request is actually updated.  At the very minimum a pull request's build should be re-executed prior to the\npull request being merged if  master  has changed since the pull request was\nopened.", 
            "title": "Review Issues &amp; Pull Requests"
        }, 
        {
            "location": "/dev-guide/release-process/#prepare-release-notes", 
            "text": "Update the release notes at  .docs/about/release-notes.md . This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.  The most recent,  stable  version of the release notes are always available\nonline at libStorage's documentation site .", 
            "title": "Prepare Release Notes"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file", 
            "text": "The  VERSION  file exists at the root of the project and should be updated to\nreflect the value of the intended release.  For example, if creating the first release candidate for version 0.1.0, the\ncontents of the  VERSION  file should be a single line  0.1.0-rc1  followed by\na newline character:  $ cat VERSION\n0.1.0-rc1  If releasing version 0.1.0 proper then the contents of the  VERSION  file\nshould be  0.1.0  followed by a newline character:  $ cat VERSION\n0.1.0", 
            "title": "Update Version File"
        }, 
        {
            "location": "/dev-guide/release-process/#commit-pull-request", 
            "text": "Once all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.  Please make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.  A release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:  Release Candidate 0.1.0-rc1\n\nThis patch marks release candidate 0.1.0-rc1.  If the commit message is longer it should simply reflect the same information\nfrom the release notes.  Once committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.", 
            "title": "Commit &amp; Pull Request"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-the-release", 
            "text": "Once the pull request marking the  staged  or  stable  release has been merged\ninto  upstream 's  master  it's time to tag the release.", 
            "title": "Tag the Release"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-format", 
            "text": "The release tag should follow a prescribed format depending upon the release\ntype:     Release Type  Tag Format  Example      staged  vMAJOR.MINOR.PATCH-rc[0-9]  v0.1.0-rc1    stable  vMAJOR.MINOR-PATCH  v0.1.0", 
            "title": "Tag Format"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-methods", 
            "text": "There are two ways to tag a release:   GitHub Releases  Command Line", 
            "title": "Tag Methods"
        }, 
        {
            "location": "/dev-guide/release-process/#command-line", 
            "text": "If tagging a release via the command line be sure to fetch the latest changes\nfrom  upstream 's  master  and either merge them into your local copy of master  or reset the local copy to reflect  upstream  prior to creating\nany tags.  The following combination of commands can be used to create a tag for\n0.1.0 Release Candidate 1:  git fetch upstream   \\\n  git checkout master   \\\n  git reset --hard upstream/master   \\\n  git tag -a -m v0.1.0-rc1 v0.1.0-rc1  The above example combines a few operations:   The first command fetches the  upstream  changes  The local  master  branch is checked out  The local  master  branch is hard reset to  upstream/master  An annotated tag is created on  master  for  v0.1.0-rc1 , or 0.1.0 Release\n     Candidate 1, with a tag message of  v0.1.0-rc1 .   Please note that the third step will erase any changes that exist only in the\nlocal  master  branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.  The above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a  staged  or stable  release. For  stable  releases the project's documentation will also be\nupdated.  Once positive everything looks good simply execute the following command to\npush the tag to the  upstream  repository:  git push upstream v0.1.0-rc1", 
            "title": "Command Line"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing to libStorage\n\n\nAn introduction to contributing to the libStorage project\n\n\n\n\nThe libStorage project welcomes, and depends, on contributions from developers\nand users in the open source community. Contributions can be made in a number of\nways, a few examples are:\n\n\n\n\nCode patches via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\n\n\nReporting an Issue\n\n\nPlease include as much detail as you can. This includes:\n\n\n\n\nThe OS type and version\n\n\nThe libStorage commit\n\n\nThe storage system in question\n\n\nA set of logs with debug-logging enabled that show the problem\n\n\n\n\nSubmitting Pull Requests\n\n\nOnce you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing-to-libstorage", 
            "text": "An introduction to contributing to the libStorage project   The libStorage project welcomes, and depends, on contributions from developers\nand users in the open source community. Contributions can be made in a number of\nways, a few examples are:   Code patches via pull requests  Documentation improvements  Bug reports and patch reviews", 
            "title": "Contributing to libStorage"
        }, 
        {
            "location": "/about/contributing/#reporting-an-issue", 
            "text": "Please include as much detail as you can. This includes:   The OS type and version  The libStorage commit  The storage system in question  A set of logs with debug-logging enabled that show the problem", 
            "title": "Reporting an Issue"
        }, 
        {
            "location": "/about/contributing/#submitting-pull-requests", 
            "text": "Once you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Submitting Pull Requests"
        }, 
        {
            "location": "/about/license/", 
            "text": "Licensing\n\n\nThe legal stuff\n\n\n\n\nlibStorage License\n\n\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#licensing", 
            "text": "The legal stuff", 
            "title": "Licensing"
        }, 
        {
            "location": "/about/license/#libstorage-license", 
            "text": "Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "libStorage License"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Release Notes\n\n\nRelease early, release often\n\n\n\n\nVersion 0.5.1 (2017/02/24)\n\n\nThis is a minor release to ensure Go1.6 compatibility.\n\n\nBug Fixes\n\n\n\n\nFittedCloud Go1.6 support (\n#444\n)\n\n\n\n\nVersion 0.5.0 (2017/02/24)\n\n\nThis is one of the largest releases in a while, including support for new\nstorage platforms, client enhancements, and more!\n\n\nNew Features\n\n\n\n\nAmazon Simple Storage Service FUSE (S3FS) support (\n#397\n, \n#409\n)\n\n\nGoogle Compute Engine Persistent Disk (GCEPD) support (\n#394\n, \n#416\n)\n\n\nDigitalOcean support (\n#392\n)\n\n\nMicrosoft Azure unmanaged disk support (\n#421\n)\n\n\nFittedCloud support (\n#408\n)\n\n\nStorage-platform specific mount/unmount support (\n#399\n)\n\n\nThe ScaleIO tool \ndrv_cfg\n is now an optional client-side dependency instead of required (\n#414\n)\n\n\nMulti-cluster support for ScaleIO (\n#420\n)\n\n\n\n\nBug Fixes\n\n\n\n\nPreemption fix (\n#413\n)\n\n\nCeph RBD monitored IP fix (\n#412\n, \n#424\n)\n\n\nCeph RBD dashes in names fix (\n#425\n)\n\n\nFix for \nlsx-OS wait\n argument count (\n#401\n)\n\n\nBuild fixes (\n#403\n)\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nChris Duchesne\n\n\nChris is my partner in crime when it comes to libStorage and REX-Ray. Without him I would have absolutely no one to take the fall for the heist I'm planning. So is Chris invaluable? Yeah, in that way, as the patsy who will do at least a dime while I'm on the beach sipping my drink, yeah, he's invaluable.\n\n\n\n\n\n\nTravis Rhoden\n\n\nTravis, or as I call him, T-Dawg, is essential to \"taking care of business.\" He comes to work to chew bubblegum and kick butt, and he leaves the gum at home!\n\n\n\n\n\n\nDan Norris\n\n\nDan \"The Man\" Norris is well known in the underground street-swimming circuit. Last year he tied Michael Phelps in the Santa Monica Sewer 120 meter medley. He would have won if not for stopping to create the DigitalOcean driver for libStorage.\n\n\n\n\n\n\nAlexey Morlang\n\n\nAs a third-chair oboe player in the Moscow orchestra it is surprising that Alexey still finds time to contribute to the project, but coming from a long line of oboligarchs (oboe playing oligarchs), it's just in his nature. As is creating storage drivers. That, and, well, playing the oboe.\n\n\n\n\n\n\nAndrey Pavlov\n\n\nThere is no Andrey. You have not met him. He does not exist. Don't look behind you. He is not there. He is writing storage drivers. Then just like that, he's vanished.\n\n\n\n\n\n\nLax Kota\n\n\nLax is a rock star in the Slack channel, helping others by answering their questions before the project's developers can take a stab. We do not want to upset him. It's rumored he beats those who upset him in order to provide inspiration for his true passion -- corporal poetry. Every punch thrown is another verse towards his masterpiece.\n\n\n\n\n\n\nJack Huang\n\n\nJack is not his job. Jack is not the amount of money he has in the bank. Jack is not the car he drives. Jack is not the clothes he wears. Jack is a supernova, accelerating at the speed of light beyond the bounds of quantifiable space and time. Jack is not the stuff above. Jack is not the stuff below. Jack is not the stuff in between. Jack is not the empty void. Jack. just. is.\n\n\n\n\n\n\n\n\nVersion 0.4.0 (2017/01/20)\n\n\nAnother exciting new feature release, this update brings with it support for\nthe Ceph RBD platform.\n\n\nNew Features\n\n\n\n\nCeph RBD Support (\n#347\n, \n#367\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFix Linux integration driver preemption (\n#391\n)\n\n\n\n\nVersion 0.3.8 (2017/01/05)\n\n\nThis is a minor bugfix release that includes a fix for volume filtering.\n\n\nBug Fixes\n\n\n\n\nFix for attachment filtering on unavailable volumes (\n#383\n)\n\n\n\n\nVersion 0.3.7 (2016/12/21)\n\n\nThis is a minor bugfix release that includes a fix for attachment querying.\n\n\nBug Fixes\n\n\n\n\nEFS security group ID fix (\n#369\n)\n\n\n\n\nVersion 0.3.6 (2016/12/13)\n\n\nThis is a minor release to update the build process so that smaller binaries\nfor embedding projects, such as REX-Ray, is supported.\n\n\nEnhancements\n\n\n\n\nDo not build Darwin executor on Travis-CI (\n#362\n)\n\n\n\n\nVersion 0.3.5 (2016/12/07)\n\n\nThis build updates the libStorage model and EBS driver to function with a\ncustom encryption key for encrypting volumes as well as includes a fix for\ndetermining an EFS instance's security groups.\n\n\nEnhancements\n\n\n\n\nCustom encryption key support (\n#355\n, \n#358\n)\n\n\nSupport for build-tag driven driver inclusion (\n#356\n)\n\n\n\n\nBug Fixes\n\n\n\n\nEFS security group ID fix (\n#354\n)\n\n\n\n\nVersion 0.3.4 (2016/12/02)\n\n\nThis is a minor release that restricts some initialization logging so\nthat it only appears if the environment variable \nLIBSTORAGE_DEBUG\n is set to a\ntruthy value.\n\n\nBug Fixes\n\n\n\n\nFix for path initialization logging (\n#349\n)\n\n\n\n\nUpdates\n\n\n\n\nUpdated build matrix (\n#350\n)\n\n\n\n\nVersion 0.3.3 (2016/11/29)\n\n\nThis release includes some minor fixes as well as a new way to query\nattachment information about one or more volumes.\n\n\nEnhancements\n\n\n\n\nEnhanced attachment querying (\n#313\n, \n#316\n, \n#319\n, \n#330\n, \n#331\n, \n#332\n, \n#334\n,\n\n#335\n, \n#336\n, \n#343\n)\n\n\n\n\nBug Fixes\n\n\n\n\nAWS Config Support (\n#314\n)\n\n\nVirtualBox Executor Fix (\n#325\n)\n\n\n\n\nVersion 0.3.2 (2016/10/18)\n\n\nThis release updates the project to reflect its new location at\ngithub.com/codedellemc.\n\n\nEnhancements\n\n\n\n\nRelocated to codedellemc (\n#307\n)\n\n\n\n\nVersion 0.3.1 (2016/10/18)\n\n\nThis is a minor update that includes support for ScaleIO 2.0.1.\n\n\nEnhancements\n\n\n\n\nSupport for ScaleIO 2.0.1 (\n#303\n)\n\n\n\n\nVersion 0.3.0 (2016/10/16)\n\n\nThis release introduces the Elastic Block Storage (EBS) driver, formerly known\nas the EC2 driver in REX-Ray \n=0.3.x.\n\n\nEnhancements\n\n\n\n\nAmazon Elastic Block Storage (EBS) Support (\n#248\n, \n#279\n)\n\n\nBuild with Docker (\n#274\n, \n#281\n)\n\n\nDocumentation updates (\n#298\n)\n\n\n\n\nBug Fixes\n\n\n\n\nVolume Removal Instance ID Fix (\n#292\n)\n\n\nAvoid Client Failure when Server Driver not Supported (\n#296\n, \n#297\n, \n#299\n, \n#300\n)\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nProud Heng\n\n\nSo long Proud, and thanks for all the fish. EBS is now part of a tagged release!\n\n\n\n\n\n\nAaron Spiegel\n\n\nAaron, you may be a new contributor, but I feel like we've known each other since we were kids, running around the front-yard on a summer's dusky-eve, catching fireflies and speaking of the day we'd be patching Markdown documentation together.\n\n\n\n\n\n\nTravis Rhoden\n\n\nWhile we've been colleagues a while, I'm thrilled you're finally working with the rest of the nerdiest of nerds, on libStorage and the secret holographic unicorn fight club we run on Thursday nights.\n\n\n\n\n\n\n\n\nVersion 0.2.1 (2016/09/14)\n\n\nThis is a minor release that includes a fix for the EFS storage driver as well\nas improvements to the build process. For example, Travis-CI now builds\nlibStorage against multiple versions of Golang and both Linux and Darwin.\n\n\nBug Fixes\n\n\n\n\nEFS Volume / Tag Creation Bug (\n#261\n)\n\n\nNext Device Fix (\n#268\n)\n\n\n\n\nEnhancements\n\n\n\n\nBuild Matrix Support (\n#263\n)\n\n\nGlide 12 Support (\n#265\n)\n\n\n\n\nVersion 0.2.0 (2016/09/07)\n\n\nBeginning with this release, libStorage's versions will increment the MINOR\ncomponent with the introduction of a new storage driver in concert with the\n\nguidelines\n set forth by semantic versioning.\n\n\nNew Features\n\n\n\n\nAmazon Elastic File System (EFS) Support (\n#231\n)\n\n\n\n\nEnhancements\n\n\n\n\nSupport for Go 1.7 (\n#251\n)\n\n\n\n\nBug Fixes\n\n\n\n\nIsilon Export Permissions (\n#252\n, \n#257\n)\n\n\nIsilon Volume Removal (\n#253\n)\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nChris Duchesne\n\n\nChris not only took on the role of project manager for libStorage and REX-Ray, he still provides ongoing test plan execution and release validation. Thank you Chris!\n\n\n\n\n\n\nKenny Cole\n\n\nKenny's tireless effort to support users and triage submitted issues is such a cornerstone to libStorage that I'm not sure what this project would do without him!\n\n\n\n\n\n\nMartin Hrabovcin\n\n\nMartin, along with Kasisnu, definitely win the \"Community Members of the Month\" award! Their hard work and dedication resulted in the introduction of the Amazon EFS storage driver. Thank you Martin \n Kasisnu!\n\n\n\n\n\n\nKasisnu Singh\n\n\nHave I mentioned we have the best community around? Seriously, thank you again Kasisnu! Your work, along with Martin's, is a milestone in the growth of libStorage.\n\n\n\n\n\n\n\n\nVersion 0.1.5 (2016/07/12)\n\n\nThis release comes hot on the heels of the last, but some dynamite bug fixes\nhave improved the performance of the server by leaps and bounds. Operations\nthat were taking minutes now take seconds or less. Memory consumption that\ncould exceed 50GB is now kept neat and tidy.\n\n\nBug Fixes\n\n\n\n\nTask service memory fix (\n#225\n)\n\n\nContext logger optimizations (\n#224\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved volume path caching (\n#227\n)\n\n\nMake Gometalinter optional (\n#223\n)\n\n\n\n\nVersion 0.1.4 (2016/07/08)\n\n\nThis update provides a major performance improvement as well as a few other,\nminor bug fixes and enhancements.\n\n\nBug Fixes\n\n\n\n\nPerformance degradation bug (\n#218\n)\n\n\nClose bug in ScaleIO driver (\n#213\n)\n\n\nPanic when checking attached instances with Isilon driver (\n#211\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved build process (\n#220\n)\n\n\nImproved executor logging (\n#217\n)\n\n\nLog timestamps in ms (\n#219\n)\n\n\nUpdated ScaleIO docs (\n#214\n)\n\n\n\n\nVersion 0.1.3 (2016/06/14)\n\n\nThis is a minor update to support the release of REX-Ray 0.4.0.\n\n\nEnhancements\n\n\n\n\nMarshal to YAML Enhancements (\n#203\n)\n\n\n\n\nVersion 0.1.2 (2016/06/13)\n\n\nThis release updates the default VirtualBox endpoint to \nhttp://10.0.2.2:18083\n.\n\n\nVersion 0.1.1 (2016/06/10)\n\n\nThis is the initial GA release of libStorage.\n\n\nFeatures\n\n\nlibStorage is an open source, platform agnostic, storage provisioning and\norchestration framework, model, and API. Features include:\n\n\n\n\nA standardized storage orchestration\n  \nmodel and API\n\n\nA lightweight, reference client implementation with a minimal dependency\n  footprint\n\n\nThe ability to embed both the libStorage client and server, creating native\n  application integration opportunities\n\n\n\n\nOperations\n\n\nlibStorage\n supports the following operations:\n\n\n\n\n\n\n\n\nResource Type\n\n\nOperation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVolume\n\n\nList / Inspect\n\n\nGet detailed information about one to many volumes\n\n\n\n\n\n\n\n\nCreate / Remote\n\n\nManage the volume lifecycle\n\n\n\n\n\n\n\n\nAttach / Detach\n\n\nProvision volumes to a client\n\n\n\n\n\n\n\n\nMount / Unmount\n\n\nMake attached volumes ready-to-use, local file systems\n\n\n\n\n\n\nSnapshot\n\n\n\n\nComing soon\n\n\n\n\n\n\nStorage Pool\n\n\n\n\nComing soon\n\n\n\n\n\n\n\n\nGetting Started\n\n\nUsing libStorage can be broken down into several, distinct steps:\n\n\n\n\nConfiguring \nlibStorage\n\n\nUnderstanding the \nAPI\n\n\nIdentifying a production server and client implementation, such as\n   \nREX-Ray\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nClint Kitson\n\n\nHis vision come to fruition. That's \nhis\n vision, thus please assign \nall\n bugs to Clint :)\n\n\n\n\n\n\nVladimir Vivien\n\n\nA nascent player, Vlad had to hit the ground running and has been a key contributor\n\n\n\n\n\n\nKenny Coleman\n\n\nWhile some come close, none are comparable to Kenny's handlebar\n\n\n\n\n\n\nJonas Rosland\n\n\nAlways good for a sanity check and keeping things on the straight and narrow\n\n\n\n\n\n\nSteph Carlson\n\n\nSteph keeps the convention train chugging along...\n\n\n\n\n\n\nAmanda Katona\n\n\nAnd Amanda is the one keeping the locomotive from going off the rails\n\n\n\n\n\n\nDrew Smith\n\n\nDrew is always ready to lend a hand, no matter the problem\n\n\n\n\n\n\nChris Duchesne\n\n\nHis short time with the team is in complete opposition to the value he has added to this project\n\n\n\n\n\n\nDavid vonThenen\n\n\nDavid has been a go-to guy for debugging the most difficult of issues\n\n\n\n\n\n\nSteve Wong\n\n\nSteve stays on top of the things and keeps use cases in sync with industry needs\n\n\n\n\n\n\nTravis Rhoden\n\n\nAnother keen mind, Travis is also a great font of technical know-how\n\n\n\n\n\n\nPeter Blum\n\n\nAbsent Peter, the EMC World demo would not have been ready\n\n\n\n\n\n\nMegan Hyland\n\n\nAnd absent Megan, Peter's work would only have taken things halfway there\n\n\n\n\n\n\nEugene Chupriyanov\n\n\nFor helping with the EC2 planning\n\n\n\n\n\n\nMatt Farina\n\n\nWithout Glide, it all comes crashing down\n\n\n\n\n\n\nJosh Bernstein\n\n\nThe shadowy figure behind the curtain...\n\n\n\n\n\n\n\n\nAnd many more...", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#release-notes", 
            "text": "Release early, release often", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#version-051-20170224", 
            "text": "This is a minor release to ensure Go1.6 compatibility.", 
            "title": "Version 0.5.1 (2017/02/24)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes", 
            "text": "FittedCloud Go1.6 support ( #444 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-050-20170224", 
            "text": "This is one of the largest releases in a while, including support for new\nstorage platforms, client enhancements, and more!", 
            "title": "Version 0.5.0 (2017/02/24)"
        }, 
        {
            "location": "/about/release-notes/#new-features", 
            "text": "Amazon Simple Storage Service FUSE (S3FS) support ( #397 ,  #409 )  Google Compute Engine Persistent Disk (GCEPD) support ( #394 ,  #416 )  DigitalOcean support ( #392 )  Microsoft Azure unmanaged disk support ( #421 )  FittedCloud support ( #408 )  Storage-platform specific mount/unmount support ( #399 )  The ScaleIO tool  drv_cfg  is now an optional client-side dependency instead of required ( #414 )  Multi-cluster support for ScaleIO ( #420 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_1", 
            "text": "Preemption fix ( #413 )  Ceph RBD monitored IP fix ( #412 ,  #424 )  Ceph RBD dashes in names fix ( #425 )  Fix for  lsx-OS wait  argument count ( #401 )  Build fixes ( #403 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you", 
            "text": "Name  Blame      Chris Duchesne  Chris is my partner in crime when it comes to libStorage and REX-Ray. Without him I would have absolutely no one to take the fall for the heist I'm planning. So is Chris invaluable? Yeah, in that way, as the patsy who will do at least a dime while I'm on the beach sipping my drink, yeah, he's invaluable.    Travis Rhoden  Travis, or as I call him, T-Dawg, is essential to \"taking care of business.\" He comes to work to chew bubblegum and kick butt, and he leaves the gum at home!    Dan Norris  Dan \"The Man\" Norris is well known in the underground street-swimming circuit. Last year he tied Michael Phelps in the Santa Monica Sewer 120 meter medley. He would have won if not for stopping to create the DigitalOcean driver for libStorage.    Alexey Morlang  As a third-chair oboe player in the Moscow orchestra it is surprising that Alexey still finds time to contribute to the project, but coming from a long line of oboligarchs (oboe playing oligarchs), it's just in his nature. As is creating storage drivers. That, and, well, playing the oboe.    Andrey Pavlov  There is no Andrey. You have not met him. He does not exist. Don't look behind you. He is not there. He is writing storage drivers. Then just like that, he's vanished.    Lax Kota  Lax is a rock star in the Slack channel, helping others by answering their questions before the project's developers can take a stab. We do not want to upset him. It's rumored he beats those who upset him in order to provide inspiration for his true passion -- corporal poetry. Every punch thrown is another verse towards his masterpiece.    Jack Huang  Jack is not his job. Jack is not the amount of money he has in the bank. Jack is not the car he drives. Jack is not the clothes he wears. Jack is a supernova, accelerating at the speed of light beyond the bounds of quantifiable space and time. Jack is not the stuff above. Jack is not the stuff below. Jack is not the stuff in between. Jack is not the empty void. Jack. just. is.", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-040-20170120", 
            "text": "Another exciting new feature release, this update brings with it support for\nthe Ceph RBD platform.", 
            "title": "Version 0.4.0 (2017/01/20)"
        }, 
        {
            "location": "/about/release-notes/#new-features_1", 
            "text": "Ceph RBD Support ( #347 ,  #367 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_2", 
            "text": "Fix Linux integration driver preemption ( #391 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-038-20170105", 
            "text": "This is a minor bugfix release that includes a fix for volume filtering.", 
            "title": "Version 0.3.8 (2017/01/05)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_3", 
            "text": "Fix for attachment filtering on unavailable volumes ( #383 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-037-20161221", 
            "text": "This is a minor bugfix release that includes a fix for attachment querying.", 
            "title": "Version 0.3.7 (2016/12/21)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_4", 
            "text": "EFS security group ID fix ( #369 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-036-20161213", 
            "text": "This is a minor release to update the build process so that smaller binaries\nfor embedding projects, such as REX-Ray, is supported.", 
            "title": "Version 0.3.6 (2016/12/13)"
        }, 
        {
            "location": "/about/release-notes/#enhancements", 
            "text": "Do not build Darwin executor on Travis-CI ( #362 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-035-20161207", 
            "text": "This build updates the libStorage model and EBS driver to function with a\ncustom encryption key for encrypting volumes as well as includes a fix for\ndetermining an EFS instance's security groups.", 
            "title": "Version 0.3.5 (2016/12/07)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_1", 
            "text": "Custom encryption key support ( #355 ,  #358 )  Support for build-tag driven driver inclusion ( #356 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_5", 
            "text": "EFS security group ID fix ( #354 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-034-20161202", 
            "text": "This is a minor release that restricts some initialization logging so\nthat it only appears if the environment variable  LIBSTORAGE_DEBUG  is set to a\ntruthy value.", 
            "title": "Version 0.3.4 (2016/12/02)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_6", 
            "text": "Fix for path initialization logging ( #349 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#updates", 
            "text": "Updated build matrix ( #350 )", 
            "title": "Updates"
        }, 
        {
            "location": "/about/release-notes/#version-033-20161129", 
            "text": "This release includes some minor fixes as well as a new way to query\nattachment information about one or more volumes.", 
            "title": "Version 0.3.3 (2016/11/29)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_2", 
            "text": "Enhanced attachment querying ( #313 ,  #316 ,  #319 ,  #330 ,  #331 ,  #332 ,  #334 , #335 ,  #336 ,  #343 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_7", 
            "text": "AWS Config Support ( #314 )  VirtualBox Executor Fix ( #325 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-032-20161018", 
            "text": "This release updates the project to reflect its new location at\ngithub.com/codedellemc.", 
            "title": "Version 0.3.2 (2016/10/18)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_3", 
            "text": "Relocated to codedellemc ( #307 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-031-20161018", 
            "text": "This is a minor update that includes support for ScaleIO 2.0.1.", 
            "title": "Version 0.3.1 (2016/10/18)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_4", 
            "text": "Support for ScaleIO 2.0.1 ( #303 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-030-20161016", 
            "text": "This release introduces the Elastic Block Storage (EBS) driver, formerly known\nas the EC2 driver in REX-Ray  =0.3.x.", 
            "title": "Version 0.3.0 (2016/10/16)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_5", 
            "text": "Amazon Elastic Block Storage (EBS) Support ( #248 ,  #279 )  Build with Docker ( #274 ,  #281 )  Documentation updates ( #298 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_8", 
            "text": "Volume Removal Instance ID Fix ( #292 )  Avoid Client Failure when Server Driver not Supported ( #296 ,  #297 ,  #299 ,  #300 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you_1", 
            "text": "Name  Blame      Proud Heng  So long Proud, and thanks for all the fish. EBS is now part of a tagged release!    Aaron Spiegel  Aaron, you may be a new contributor, but I feel like we've known each other since we were kids, running around the front-yard on a summer's dusky-eve, catching fireflies and speaking of the day we'd be patching Markdown documentation together.    Travis Rhoden  While we've been colleagues a while, I'm thrilled you're finally working with the rest of the nerdiest of nerds, on libStorage and the secret holographic unicorn fight club we run on Thursday nights.", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-021-20160914", 
            "text": "This is a minor release that includes a fix for the EFS storage driver as well\nas improvements to the build process. For example, Travis-CI now builds\nlibStorage against multiple versions of Golang and both Linux and Darwin.", 
            "title": "Version 0.2.1 (2016/09/14)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_9", 
            "text": "EFS Volume / Tag Creation Bug ( #261 )  Next Device Fix ( #268 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#enhancements_6", 
            "text": "Build Matrix Support ( #263 )  Glide 12 Support ( #265 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-020-20160907", 
            "text": "Beginning with this release, libStorage's versions will increment the MINOR\ncomponent with the introduction of a new storage driver in concert with the guidelines  set forth by semantic versioning.", 
            "title": "Version 0.2.0 (2016/09/07)"
        }, 
        {
            "location": "/about/release-notes/#new-features_2", 
            "text": "Amazon Elastic File System (EFS) Support ( #231 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_7", 
            "text": "Support for Go 1.7 ( #251 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_10", 
            "text": "Isilon Export Permissions ( #252 ,  #257 )  Isilon Volume Removal ( #253 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you_2", 
            "text": "Name  Blame      Chris Duchesne  Chris not only took on the role of project manager for libStorage and REX-Ray, he still provides ongoing test plan execution and release validation. Thank you Chris!    Kenny Cole  Kenny's tireless effort to support users and triage submitted issues is such a cornerstone to libStorage that I'm not sure what this project would do without him!    Martin Hrabovcin  Martin, along with Kasisnu, definitely win the \"Community Members of the Month\" award! Their hard work and dedication resulted in the introduction of the Amazon EFS storage driver. Thank you Martin   Kasisnu!    Kasisnu Singh  Have I mentioned we have the best community around? Seriously, thank you again Kasisnu! Your work, along with Martin's, is a milestone in the growth of libStorage.", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-015-20160712", 
            "text": "This release comes hot on the heels of the last, but some dynamite bug fixes\nhave improved the performance of the server by leaps and bounds. Operations\nthat were taking minutes now take seconds or less. Memory consumption that\ncould exceed 50GB is now kept neat and tidy.", 
            "title": "Version 0.1.5 (2016/07/12)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_11", 
            "text": "Task service memory fix ( #225 )  Context logger optimizations ( #224 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#enhancements_8", 
            "text": "Improved volume path caching ( #227 )  Make Gometalinter optional ( #223 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-014-20160708", 
            "text": "This update provides a major performance improvement as well as a few other,\nminor bug fixes and enhancements.", 
            "title": "Version 0.1.4 (2016/07/08)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_12", 
            "text": "Performance degradation bug ( #218 )  Close bug in ScaleIO driver ( #213 )  Panic when checking attached instances with Isilon driver ( #211 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#enhancements_9", 
            "text": "Improved build process ( #220 )  Improved executor logging ( #217 )  Log timestamps in ms ( #219 )  Updated ScaleIO docs ( #214 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-013-20160614", 
            "text": "This is a minor update to support the release of REX-Ray 0.4.0.", 
            "title": "Version 0.1.3 (2016/06/14)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_10", 
            "text": "Marshal to YAML Enhancements ( #203 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-012-20160613", 
            "text": "This release updates the default VirtualBox endpoint to  http://10.0.2.2:18083 .", 
            "title": "Version 0.1.2 (2016/06/13)"
        }, 
        {
            "location": "/about/release-notes/#version-011-20160610", 
            "text": "This is the initial GA release of libStorage.", 
            "title": "Version 0.1.1 (2016/06/10)"
        }, 
        {
            "location": "/about/release-notes/#features", 
            "text": "libStorage is an open source, platform agnostic, storage provisioning and\norchestration framework, model, and API. Features include:   A standardized storage orchestration\n   model and API  A lightweight, reference client implementation with a minimal dependency\n  footprint  The ability to embed both the libStorage client and server, creating native\n  application integration opportunities", 
            "title": "Features"
        }, 
        {
            "location": "/about/release-notes/#operations", 
            "text": "libStorage  supports the following operations:     Resource Type  Operation  Description      Volume  List / Inspect  Get detailed information about one to many volumes     Create / Remote  Manage the volume lifecycle     Attach / Detach  Provision volumes to a client     Mount / Unmount  Make attached volumes ready-to-use, local file systems    Snapshot   Coming soon    Storage Pool   Coming soon", 
            "title": "Operations"
        }, 
        {
            "location": "/about/release-notes/#getting-started", 
            "text": "Using libStorage can be broken down into several, distinct steps:   Configuring  libStorage  Understanding the  API  Identifying a production server and client implementation, such as\n    REX-Ray", 
            "title": "Getting Started"
        }, 
        {
            "location": "/about/release-notes/#thank-you_3", 
            "text": "Name  Blame      Clint Kitson  His vision come to fruition. That's  his  vision, thus please assign  all  bugs to Clint :)    Vladimir Vivien  A nascent player, Vlad had to hit the ground running and has been a key contributor    Kenny Coleman  While some come close, none are comparable to Kenny's handlebar    Jonas Rosland  Always good for a sanity check and keeping things on the straight and narrow    Steph Carlson  Steph keeps the convention train chugging along...    Amanda Katona  And Amanda is the one keeping the locomotive from going off the rails    Drew Smith  Drew is always ready to lend a hand, no matter the problem    Chris Duchesne  His short time with the team is in complete opposition to the value he has added to this project    David vonThenen  David has been a go-to guy for debugging the most difficult of issues    Steve Wong  Steve stays on top of the things and keeps use cases in sync with industry needs    Travis Rhoden  Another keen mind, Travis is also a great font of technical know-how    Peter Blum  Absent Peter, the EMC World demo would not have been ready    Megan Hyland  And absent Megan, Peter's work would only have taken things halfway there    Eugene Chupriyanov  For helping with the EC2 planning    Matt Farina  Without Glide, it all comes crashing down    Josh Bernstein  The shadowy figure behind the curtain...     And many more...", 
            "title": "Thank You"
        }
    ]
}